name: 'Single Shot Mask - ResNet 50'

layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "scaleMask_16"
  top: "scaleMask_32"
  top: "scaleMask_64"
  top: "scaleMask_128"
#  top: "masks"
#  top: "bbs"
#  top: "bbs_hw"
#  top: "options"
#  top: "configs"
  top: "id"
  python_param {
    module: "data.layers"
    layer: "COCOSSMSpiderMultiAttSize128"
  }
}

layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "bn_conv1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale_conv1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_relu"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
    
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    
  }
}
layer {
  name: "res2a_branch1"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch1"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn2a_branch1"
  type: "BatchNorm"
  bottom: "res2a_branch1"
  top: "res2a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2a_branch1"
  type: "Scale"
  bottom: "res2a_branch1"
  top: "res2a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn2a_branch2a"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2a_branch2a"
  type: "Scale"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2a_branch2a_relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
    
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn2a_branch2b"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2a_branch2b"
  type: "Scale"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2a_branch2b_relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
    
}
layer {
  name: "res2a_branch2c"
  type: "Convolution"
  bottom: "res2a_branch2b"
  top: "res2a_branch2c"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn2a_branch2c"
  type: "BatchNorm"
  bottom: "res2a_branch2c"
  top: "res2a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2a_branch2c"
  type: "Scale"
  bottom: "res2a_branch2c"
  top: "res2a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2a"
  type: "Eltwise"
  bottom: "res2a_branch1"
  bottom: "res2a_branch2c"
  top: "res2a"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2a_relu"
  type: "ReLU"
  bottom: "res2a"
  top: "res2a"
    
}
layer {
  name: "res2b_branch2a"
  type: "Convolution"
  bottom: "res2a"
  top: "res2b_branch2a"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn2b_branch2a"
  type: "BatchNorm"
  bottom: "res2b_branch2a"
  top: "res2b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2b_branch2a"
  type: "Scale"
  bottom: "res2b_branch2a"
  top: "res2b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2b_branch2a_relu"
  type: "ReLU"
  bottom: "res2b_branch2a"
  top: "res2b_branch2a"
    
}
layer {
  name: "res2b_branch2b"
  type: "Convolution"
  bottom: "res2b_branch2a"
  top: "res2b_branch2b"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn2b_branch2b"
  type: "BatchNorm"
  bottom: "res2b_branch2b"
  top: "res2b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2b_branch2b"
  type: "Scale"
  bottom: "res2b_branch2b"
  top: "res2b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2b_branch2b_relu"
  type: "ReLU"
  bottom: "res2b_branch2b"
  top: "res2b_branch2b"
    
}
layer {
  name: "res2b_branch2c"
  type: "Convolution"
  bottom: "res2b_branch2b"
  top: "res2b_branch2c"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn2b_branch2c"
  type: "BatchNorm"
  bottom: "res2b_branch2c"
  top: "res2b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2b_branch2c"
  type: "Scale"
  bottom: "res2b_branch2c"
  top: "res2b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2b"
  type: "Eltwise"
  bottom: "res2a"
  bottom: "res2b_branch2c"
  top: "res2b"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2b_relu"
  type: "ReLU"
  bottom: "res2b"
  top: "res2b"
    
}
layer {
  name: "res2c_branch2a"
  type: "Convolution"
  bottom: "res2b"
  top: "res2c_branch2a"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn2c_branch2a"
  type: "BatchNorm"
  bottom: "res2c_branch2a"
  top: "res2c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2c_branch2a"
  type: "Scale"
  bottom: "res2c_branch2a"
  top: "res2c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2c_branch2a_relu"
  type: "ReLU"
  bottom: "res2c_branch2a"
  top: "res2c_branch2a"
    
}
layer {
  name: "res2c_branch2b"
  type: "Convolution"
  bottom: "res2c_branch2a"
  top: "res2c_branch2b"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn2c_branch2b"
  type: "BatchNorm"
  bottom: "res2c_branch2b"
  top: "res2c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2c_branch2b"
  type: "Scale"
  bottom: "res2c_branch2b"
  top: "res2c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2c_branch2b_relu"
  type: "ReLU"
  bottom: "res2c_branch2b"
  top: "res2c_branch2b"
    
}
layer {
  name: "res2c_branch2c"
  type: "Convolution"
  bottom: "res2c_branch2b"
  top: "res2c_branch2c"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn2c_branch2c"
  type: "BatchNorm"
  bottom: "res2c_branch2c"
  top: "res2c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2c_branch2c"
  type: "Scale"
  bottom: "res2c_branch2c"
  top: "res2c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2c"
  type: "Eltwise"
  bottom: "res2b"
  bottom: "res2c_branch2c"
  top: "res2c"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2c_relu"
  type: "ReLU"
  bottom: "res2c"
  top: "res2c"
    
}
layer {
  name: "res3a_branch1"
  type: "Convolution"
  bottom: "res2c"
  top: "res3a_branch1"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    
  }
}
layer {
  name: "bn3a_branch1"
  type: "BatchNorm"
  bottom: "res3a_branch1"
  top: "res3a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3a_branch1"
  type: "Scale"
  bottom: "res3a_branch1"
  top: "res3a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "res2c"
  top: "res3a_branch2a"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    
  }
}
layer {
  name: "bn3a_branch2a"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3a_branch2a"
  type: "Scale"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3a_branch2a_relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
    
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn3a_branch2b"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3a_branch2b"
  type: "Scale"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3a_branch2b_relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
    
}
layer {
  name: "res3a_branch2c"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "res3a_branch2c"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn3a_branch2c"
  type: "BatchNorm"
  bottom: "res3a_branch2c"
  top: "res3a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3a_branch2c"
  type: "Scale"
  bottom: "res3a_branch2c"
  top: "res3a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3a"
  type: "Eltwise"
  bottom: "res3a_branch1"
  bottom: "res3a_branch2c"
  top: "res3a"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3a_relu"
  type: "ReLU"
  bottom: "res3a"
  top: "res3a"
    
}
layer {
  name: "res3b_branch2a"
  type: "Convolution"
  bottom: "res3a"
  top: "res3b_branch2a"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn3b_branch2a"
  type: "BatchNorm"
  bottom: "res3b_branch2a"
  top: "res3b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3b_branch2a"
  type: "Scale"
  bottom: "res3b_branch2a"
  top: "res3b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3b_branch2a_relu"
  type: "ReLU"
  bottom: "res3b_branch2a"
  top: "res3b_branch2a"
    
}
layer {
  name: "res3b_branch2b"
  type: "Convolution"
  bottom: "res3b_branch2a"
  top: "res3b_branch2b"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn3b_branch2b"
  type: "BatchNorm"
  bottom: "res3b_branch2b"
  top: "res3b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3b_branch2b"
  type: "Scale"
  bottom: "res3b_branch2b"
  top: "res3b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3b_branch2b_relu"
  type: "ReLU"
  bottom: "res3b_branch2b"
  top: "res3b_branch2b"
    
}
layer {
  name: "res3b_branch2c"
  type: "Convolution"
  bottom: "res3b_branch2b"
  top: "res3b_branch2c"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn3b_branch2c"
  type: "BatchNorm"
  bottom: "res3b_branch2c"
  top: "res3b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3b_branch2c"
  type: "Scale"
  bottom: "res3b_branch2c"
  top: "res3b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3b"
  type: "Eltwise"
  bottom: "res3a"
  bottom: "res3b_branch2c"
  top: "res3b"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3b_relu"
  type: "ReLU"
  bottom: "res3b"
  top: "res3b"
    
}
layer {
  name: "res3c_branch2a"
  type: "Convolution"
  bottom: "res3b"
  top: "res3c_branch2a"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn3c_branch2a"
  type: "BatchNorm"
  bottom: "res3c_branch2a"
  top: "res3c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3c_branch2a"
  type: "Scale"
  bottom: "res3c_branch2a"
  top: "res3c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3c_branch2a_relu"
  type: "ReLU"
  bottom: "res3c_branch2a"
  top: "res3c_branch2a"
    
}
layer {
  name: "res3c_branch2b"
  type: "Convolution"
  bottom: "res3c_branch2a"
  top: "res3c_branch2b"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn3c_branch2b"
  type: "BatchNorm"
  bottom: "res3c_branch2b"
  top: "res3c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3c_branch2b"
  type: "Scale"
  bottom: "res3c_branch2b"
  top: "res3c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3c_branch2b_relu"
  type: "ReLU"
  bottom: "res3c_branch2b"
  top: "res3c_branch2b"
    
}
layer {
  name: "res3c_branch2c"
  type: "Convolution"
  bottom: "res3c_branch2b"
  top: "res3c_branch2c"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn3c_branch2c"
  type: "BatchNorm"
  bottom: "res3c_branch2c"
  top: "res3c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3c_branch2c"
  type: "Scale"
  bottom: "res3c_branch2c"
  top: "res3c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3c"
  type: "Eltwise"
  bottom: "res3b"
  bottom: "res3c_branch2c"
  top: "res3c"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3c_relu"
  type: "ReLU"
  bottom: "res3c"
  top: "res3c"
    
}
layer {
  name: "res3d_branch2a"
  type: "Convolution"
  bottom: "res3c"
  top: "res3d_branch2a"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn3d_branch2a"
  type: "BatchNorm"
  bottom: "res3d_branch2a"
  top: "res3d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3d_branch2a"
  type: "Scale"
  bottom: "res3d_branch2a"
  top: "res3d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3d_branch2a_relu"
  type: "ReLU"
  bottom: "res3d_branch2a"
  top: "res3d_branch2a"
    
}
layer {
  name: "res3d_branch2b"
  type: "Convolution"
  bottom: "res3d_branch2a"
  top: "res3d_branch2b"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn3d_branch2b"
  type: "BatchNorm"
  bottom: "res3d_branch2b"
  top: "res3d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3d_branch2b"
  type: "Scale"
  bottom: "res3d_branch2b"
  top: "res3d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3d_branch2b_relu"
  type: "ReLU"
  bottom: "res3d_branch2b"
  top: "res3d_branch2b"
    
}
layer {
  name: "res3d_branch2c"
  type: "Convolution"
  bottom: "res3d_branch2b"
  top: "res3d_branch2c"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn3d_branch2c"
  type: "BatchNorm"
  bottom: "res3d_branch2c"
  top: "res3d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3d_branch2c"
  type: "Scale"
  bottom: "res3d_branch2c"
  top: "res3d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3d"
  type: "Eltwise"
  bottom: "res3c"
  bottom: "res3d_branch2c"
  top: "res3d"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3d_relu"
  type: "ReLU"
  bottom: "res3d"
  top: "res3d"
    
}

################ div2 branch ##############
layer {
  name: "res4a_branch1"
  type: "Convolution"
  bottom: "res3d"
  top: "res4a_branch1"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    
  }
}
layer {
  name: "bn4a_branch1"
  type: "BatchNorm"
  bottom: "res4a_branch1"
  top: "res4a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch1"
  type: "Scale"
  bottom: "res4a_branch1"
  top: "res4a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "res3d"
  top: "res4a_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    
  }
}
layer {
  name: "bn4a_branch2a"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch2a"
  type: "Scale"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_branch2a_relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
    
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn4a_branch2b"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch2b"
  type: "Scale"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_branch2b_relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
    
}
layer {
  name: "res4a_branch2c"
  type: "Convolution"
  bottom: "res4a_branch2b"
  top: "res4a_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4a_branch2c"
  type: "BatchNorm"
  bottom: "res4a_branch2c"
  top: "res4a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch2c"
  type: "Scale"
  bottom: "res4a_branch2c"
  top: "res4a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a"
  type: "Eltwise"
  bottom: "res4a_branch1"
  bottom: "res4a_branch2c"
  top: "res4a"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4a_relu"
  type: "ReLU"
  bottom: "res4a"
  top: "res4a"
    
}
layer {
  name: "res4b_branch2a"
  type: "Convolution"
  bottom: "res4a"
  top: "res4b_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4b_branch2a"
  type: "BatchNorm"
  bottom: "res4b_branch2a"
  top: "res4b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4b_branch2a"
  type: "Scale"
  bottom: "res4b_branch2a"
  top: "res4b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4b_branch2a_relu"
  type: "ReLU"
  bottom: "res4b_branch2a"
  top: "res4b_branch2a"
    
}
layer {
  name: "res4b_branch2b"
  type: "Convolution"
  bottom: "res4b_branch2a"
  top: "res4b_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn4b_branch2b"
  type: "BatchNorm"
  bottom: "res4b_branch2b"
  top: "res4b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4b_branch2b"
  type: "Scale"
  bottom: "res4b_branch2b"
  top: "res4b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4b_branch2b_relu"
  type: "ReLU"
  bottom: "res4b_branch2b"
  top: "res4b_branch2b"
    
}
layer {
  name: "res4b_branch2c"
  type: "Convolution"
  bottom: "res4b_branch2b"
  top: "res4b_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4b_branch2c"
  type: "BatchNorm"
  bottom: "res4b_branch2c"
  top: "res4b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4b_branch2c"
  type: "Scale"
  bottom: "res4b_branch2c"
  top: "res4b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4b"
  type: "Eltwise"
  bottom: "res4a"
  bottom: "res4b_branch2c"
  top: "res4b"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4b_relu"
  type: "ReLU"
  bottom: "res4b"
  top: "res4b"
    
}
layer {
  name: "res4c_branch2a"
  type: "Convolution"
  bottom: "res4b"
  top: "res4c_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4c_branch2a"
  type: "BatchNorm"
  bottom: "res4c_branch2a"
  top: "res4c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4c_branch2a"
  type: "Scale"
  bottom: "res4c_branch2a"
  top: "res4c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4c_branch2a_relu"
  type: "ReLU"
  bottom: "res4c_branch2a"
  top: "res4c_branch2a"
    
}
layer {
  name: "res4c_branch2b"
  type: "Convolution"
  bottom: "res4c_branch2a"
  top: "res4c_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn4c_branch2b"
  type: "BatchNorm"
  bottom: "res4c_branch2b"
  top: "res4c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4c_branch2b"
  type: "Scale"
  bottom: "res4c_branch2b"
  top: "res4c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4c_branch2b_relu"
  type: "ReLU"
  bottom: "res4c_branch2b"
  top: "res4c_branch2b"
    
}
layer {
  name: "res4c_branch2c"
  type: "Convolution"
  bottom: "res4c_branch2b"
  top: "res4c_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4c_branch2c"
  type: "BatchNorm"
  bottom: "res4c_branch2c"
  top: "res4c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4c_branch2c"
  type: "Scale"
  bottom: "res4c_branch2c"
  top: "res4c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4c"
  type: "Eltwise"
  bottom: "res4b"
  bottom: "res4c_branch2c"
  top: "res4c"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4c_relu"
  type: "ReLU"
  bottom: "res4c"
  top: "res4c"
    
}
layer {
  name: "res4d_branch2a"
  type: "Convolution"
  bottom: "res4c"
  top: "res4d_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4d_branch2a"
  type: "BatchNorm"
  bottom: "res4d_branch2a"
  top: "res4d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4d_branch2a"
  type: "Scale"
  bottom: "res4d_branch2a"
  top: "res4d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4d_branch2a_relu"
  type: "ReLU"
  bottom: "res4d_branch2a"
  top: "res4d_branch2a"
    
}
layer {
  name: "res4d_branch2b"
  type: "Convolution"
  bottom: "res4d_branch2a"
  top: "res4d_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn4d_branch2b"
  type: "BatchNorm"
  bottom: "res4d_branch2b"
  top: "res4d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4d_branch2b"
  type: "Scale"
  bottom: "res4d_branch2b"
  top: "res4d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4d_branch2b_relu"
  type: "ReLU"
  bottom: "res4d_branch2b"
  top: "res4d_branch2b"
    
}
layer {
  name: "res4d_branch2c"
  type: "Convolution"
  bottom: "res4d_branch2b"
  top: "res4d_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4d_branch2c"
  type: "BatchNorm"
  bottom: "res4d_branch2c"
  top: "res4d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4d_branch2c"
  type: "Scale"
  bottom: "res4d_branch2c"
  top: "res4d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4d"
  type: "Eltwise"
  bottom: "res4c"
  bottom: "res4d_branch2c"
  top: "res4d"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4d_relu"
  type: "ReLU"
  bottom: "res4d"
  top: "res4d"
    
}
layer {
  name: "res4e_branch2a"
  type: "Convolution"
  bottom: "res4d"
  top: "res4e_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4e_branch2a"
  type: "BatchNorm"
  bottom: "res4e_branch2a"
  top: "res4e_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4e_branch2a"
  type: "Scale"
  bottom: "res4e_branch2a"
  top: "res4e_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4e_branch2a_relu"
  type: "ReLU"
  bottom: "res4e_branch2a"
  top: "res4e_branch2a"
    
}
layer {
  name: "res4e_branch2b"
  type: "Convolution"
  bottom: "res4e_branch2a"
  top: "res4e_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn4e_branch2b"
  type: "BatchNorm"
  bottom: "res4e_branch2b"
  top: "res4e_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4e_branch2b"
  type: "Scale"
  bottom: "res4e_branch2b"
  top: "res4e_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4e_branch2b_relu"
  type: "ReLU"
  bottom: "res4e_branch2b"
  top: "res4e_branch2b"
    
}
layer {
  name: "res4e_branch2c"
  type: "Convolution"
  bottom: "res4e_branch2b"
  top: "res4e_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4e_branch2c"
  type: "BatchNorm"
  bottom: "res4e_branch2c"
  top: "res4e_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4e_branch2c"
  type: "Scale"
  bottom: "res4e_branch2c"
  top: "res4e_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4e"
  type: "Eltwise"
  bottom: "res4d"
  bottom: "res4e_branch2c"
  top: "res4e"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4e_relu"
  type: "ReLU"
  bottom: "res4e"
  top: "res4e"
    
}
layer {
  name: "res4f_branch2a"
  type: "Convolution"
  bottom: "res4e"
  top: "res4f_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4f_branch2a"
  type: "BatchNorm"
  bottom: "res4f_branch2a"
  top: "res4f_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4f_branch2a"
  type: "Scale"
  bottom: "res4f_branch2a"
  top: "res4f_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4f_branch2a_relu"
  type: "ReLU"
  bottom: "res4f_branch2a"
  top: "res4f_branch2a"
    
}
layer {
  name: "res4f_branch2b"
  type: "Convolution"
  bottom: "res4f_branch2a"
  top: "res4f_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    
  }
}
layer {
  name: "bn4f_branch2b"
  type: "BatchNorm"
  bottom: "res4f_branch2b"
  top: "res4f_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4f_branch2b"
  type: "Scale"
  bottom: "res4f_branch2b"
  top: "res4f_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4f_branch2b_relu"
  type: "ReLU"
  bottom: "res4f_branch2b"
  top: "res4f_branch2b"
    
}
layer {
  name: "res4f_branch2c"
  type: "Convolution"
  bottom: "res4f_branch2b"
  top: "res4f_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    
  }
}
layer {
  name: "bn4f_branch2c"
  type: "BatchNorm"
  bottom: "res4f_branch2c"
  top: "res4f_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4f_branch2c"
  type: "Scale"
  bottom: "res4f_branch2c"
  top: "res4f_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4f"
  type: "Eltwise"
  bottom: "res4e"
  bottom: "res4f_branch2c"
  top: "res4f"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4f_relu"
  type: "ReLU"
  bottom: "res4f"
  top: "res4f"
    
}

layer {
  name: "RONres5a_branch1"
  type: "Convolution"
  bottom: "res4f"
  top: "res5a_branch1"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
  }
}
layer {
  name: "RONbn5a_branch1"
  type: "BatchNorm"
  bottom: "res5a_branch1"
  top: "res5a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale5a_branch1"
  type: "Scale"
  bottom: "res5a_branch1"
  top: "res5a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres5a_branch5a"
  type: "Convolution"
  bottom: "res4f"
  top: "res5a_branch5a"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "RONbn5a_branch5a"
  type: "BatchNorm"
  bottom: "res5a_branch5a"
  top: "res5a_branch5a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale5a_branch5a"
  type: "Scale"
  bottom: "res5a_branch5a"
  top: "res5a_branch5a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres5a_branch5a_relu"
  type: "ReLU"
  bottom: "res5a_branch5a"
  top: "res5a_branch5a"
}
layer {
  name: "RONres5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch5a"
  top: "res5a_branch2b"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "RONbn5a_branch2b"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale5a_branch2b"
  type: "Scale"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres5a_branch2b_relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "RONres5a_branch2c"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "res5a_branch2c"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "RONbn5a_branch2c"
  type: "BatchNorm"
  bottom: "res5a_branch2c"
  top: "res5a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale5a_branch2c"
  type: "Scale"
  bottom: "res5a_branch2c"
  top: "res5a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres5a"
  type: "Eltwise"
  bottom: "res5a_branch1"
  bottom: "res5a_branch2c"
  top: "res5a"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "RONres5a_relu"
  type: "ReLU"
  bottom: "res5a"
  top: "res5a"
}


layer {
  name: "RONres6a_branch1"
  type: "Convolution"
  bottom: "res5a"
  top: "res6a_branch1"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
  }
}
layer {
  name: "RONbn6a_branch1"
  type: "BatchNorm"
  bottom: "res6a_branch1"
  top: "res6a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale6a_branch1"
  type: "Scale"
  bottom: "res6a_branch1"
  top: "res6a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres6a_branch6a"
  type: "Convolution"
  bottom: "res5a"
  top: "res6a_branch6a"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "RONbn6a_branch6a"
  type: "BatchNorm"
  bottom: "res6a_branch6a"
  top: "res6a_branch6a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale6a_branch6a"
  type: "Scale"
  bottom: "res6a_branch6a"
  top: "res6a_branch6a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres6a_branch6a_relu"
  type: "ReLU"
  bottom: "res6a_branch6a"
  top: "res6a_branch6a"
}
layer {
  name: "RONres6a_branch2b"
  type: "Convolution"
  bottom: "res6a_branch6a"
  top: "res6a_branch2b"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "RONbn6a_branch2b"
  type: "BatchNorm"
  bottom: "res6a_branch2b"
  top: "res6a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale6a_branch2b"
  type: "Scale"
  bottom: "res6a_branch2b"
  top: "res6a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres6a_branch2b_relu"
  type: "ReLU"
  bottom: "res6a_branch2b"
  top: "res6a_branch2b"
}
layer {
  name: "RONres6a_branch2c"
  type: "Convolution"
  bottom: "res6a_branch2b"
  top: "res6a_branch2c"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "RONbn6a_branch2c"
  type: "BatchNorm"
  bottom: "res6a_branch2c"
  top: "res6a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale6a_branch2c"
  type: "Scale"
  bottom: "res6a_branch2c"
  top: "res6a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres6a"
  type: "Eltwise"
  bottom: "res6a_branch1"
  bottom: "res6a_branch2c"
  top: "res6a"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "RONres6a_relu"
  type: "ReLU"
  bottom: "res6a"
  top: "res6a"
}

layer {
  name: "RONres7a_branch1"
  type: "Convolution"
  bottom: "res6a"
  top: "res7a_branch1"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
  }
}
layer {
  name: "RONbn7a_branch1"
  type: "BatchNorm"
  bottom: "res7a_branch1"
  top: "res7a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale7a_branch1"
  type: "Scale"
  bottom: "res7a_branch1"
  top: "res7a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres7a_branch7a"
  type: "Convolution"
  bottom: "res6a"
  top: "res7a_branch7a"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "RONbn7a_branch7a"
  type: "BatchNorm"
  bottom: "res7a_branch7a"
  top: "res7a_branch7a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale7a_branch7a"
  type: "Scale"
  bottom: "res7a_branch7a"
  top: "res7a_branch7a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres7a_branch7a_relu"
  type: "ReLU"
  bottom: "res7a_branch7a"
  top: "res7a_branch7a"
}
layer {
  name: "RONres7a_branch2b"
  type: "Convolution"
  bottom: "res7a_branch7a"
  top: "res7a_branch2b"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "RONbn7a_branch2b"
  type: "BatchNorm"
  bottom: "res7a_branch2b"
  top: "res7a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale7a_branch2b"
  type: "Scale"
  bottom: "res7a_branch2b"
  top: "res7a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres7a_branch2b_relu"
  type: "ReLU"
  bottom: "res7a_branch2b"
  top: "res7a_branch2b"
}
layer {
  name: "RONres7a_branch2c"
  type: "Convolution"
  bottom: "res7a_branch2b"
  top: "res7a_branch2c"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "RONbn7a_branch2c"
  type: "BatchNorm"
  bottom: "res7a_branch2c"
  top: "res7a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "RONscale7a_branch2c"
  type: "Scale"
  bottom: "res7a_branch2c"
  top: "res7a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "RONres7a"
  type: "Eltwise"
  bottom: "res7a_branch1"
  bottom: "res7a_branch2c"
  top: "res7a"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "RONres7a_relu"
  type: "ReLU"
  bottom: "res7a"
  top: "res7a"
}


#res4f, res5a, res6a, res7a

layer {  name: "conv_7e_branch1"  type: "Convolution"  bottom: "res7a"  top: "res7g"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 1024  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }    }
}


layer {  name: "conv_6e_branch1"  type: "Convolution"  bottom: "res6a"  top: "res6e_branch1"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 1024  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }    }
}


layer { name: 'deconv_6e_branch2' type: "Deconvolution" bottom: "res7g" top: "res6e_branch2"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param { num_output: 1024 kernel_size: 2 stride: 2  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }    } 
}

layer {
  name: "elt_6f"
  type: "Eltwise"
  bottom: "res6e_branch1"
  bottom: "res6e_branch2"
  top: "res6g"
  eltwise_param {
    operation: SUM
  }
}

layer {  name: "conv_5e_branch1"  type: "Convolution"  bottom: "res5a"  top: "res5e_branch1"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 1024  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }    }
}


layer { name: 'deconv_5e_branch2' type: "Deconvolution" bottom: "res6g" top: "res5e_branch2"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param { num_output: 1024 kernel_size: 2 stride: 2  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }    } 
}

layer {
  name: "elt_5f"
  type: "Eltwise"
  bottom: "res5e_branch1"
  bottom: "res5e_branch2"
  top: "res5g"
  eltwise_param {
    operation: SUM
  }
}


layer {  name: "conv_4e_branch1"  type: "Convolution"  bottom: "res4a"  top: "res4e_branch1"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 1024  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }    }
}


layer { name: 'deconv_4e_branch2' type: "Deconvolution" bottom: "res5g" top: "res4e_branch2"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param { num_output: 1024 kernel_size: 2 stride: 2  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }    } 
}

layer {
  name: "elt_4f"
  type: "Eltwise"
  bottom: "res4e_branch1"
  bottom: "res4e_branch2"
  top: "res4g"
  eltwise_param {
    operation: SUM
  }
}



######### attSize 16 ##########


layer {  name: "res_buAttSize1a_16"  type: "Convolution"  bottom: "res4g"  top: "res_buAttSize1a_16"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }      }
}

layer { name: "loss_buAttSize_prec_16" type: "InfogainLoss" bottom: "res_buAttSize1a_16" bottom: "scaleMask_16" propagate_down: true propagate_down: false top: "loss_attSize_prec_16" loss_weight: 1 infogain_loss_param{ source: "infogainH_16_2_800.binaryproto" }}

layer { name: "loss_buAttSize_16" type: "Softmax" bottom: "res_buAttSize1a_16" top: "finalAtt16" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt16"
  top: "nonObj16"
  top: "obj16"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_16"
#  top: "obj16"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#bias_term: true
#    bias_filler {
#      value: 1
#    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_16"
#  top: "nonObj16"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#  }
#}

#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_16"
#  top: "obj16"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 1    }
# }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_16"
#  top: "scaleMask_16_neg"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: -1    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_16"
#  top: "scaleMask_16_1"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#bias_term: true
#   bias_filler {
#      value: 1
#    }
#  }
#}
#layer { name: "el16" type: "Eltwise" bottom: "scaleMask_16_1" bottom: "scaleMask_16_neg" top: "nonObj16" }


layer{
  type: "Silence"
#  bottom: "masks"
#  bottom: "bbs"
#  bottom: "bbs_hw"
#  bottom: "options"
#  bottom: "configs"
  bottom: "obj16"
#  bottom: "finalAtt16"
}

layer{
  type: "Reshape"
  bottom: "obj16"
  top: "obj16_flat"
  reshape_param {
      shape {
        dim: 1  # copy the dimension from below
        dim: 1
        dim: 1
        dim: -1 # infer it from the other dimensions
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj16"
  top: "nonObj16_flat"
  reshape_param {
      shape {
        dim: 1  # copy the dimension from below
        dim: 1
        dim: 1
        dim: -1 # infer it from the other dimensions
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj16_flat"
  bottom: "nonObj16_flat"
  top: "obj16_checked"
  top: "nonObj16_checked"
}

######### attSize 32 ##########

layer {  name: "res_buAttSize1a_32"  type: "Convolution"  bottom: "res5g"  top: "res_buAttSize1a_32"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }      }
}

layer { name: "loss_buAttSize_prec_32" type: "InfogainLoss" bottom: "res_buAttSize1a_32" bottom: "scaleMask_32" propagate_down: true propagate_down: false top: "loss_attSize_prec_32" loss_weight: 1 infogain_loss_param{ source: "infogainH_32_2_800.binaryproto" }}

layer { name: "loss_buAttSize_32" type: "Softmax" bottom: "res_buAttSize1a_32" top: "finalAtt32" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt32"
  top: "nonObj32"
  top: "obj32"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_32"
#  top: "obj32"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#bias_term: true
#    bias_filler {
#      value: 1
#    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_32"
#  top: "nonObj32"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#  }
#}

#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_32"
#  top: "obj32"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 1    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_32"
#  top: "scaleMask_32_neg"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: -1    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_32"
#  top: "scaleMask_32_1"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#bias_term: true
#    bias_filler {
#      value: 1
#    }
#  }
#}
#layer { name: "el16" type: "Eltwise" bottom: "scaleMask_32_1" bottom: "scaleMask_32_neg" top: "nonObj32" }

layer{
  type: "Silence"
#  bottom: "masks"
#  bottom: "bbs"
#  bottom: "bbs_hw"
#  bottom: "options"
#  bottom: "configs"
  bottom: "nonObj32"
#  bottom: "finalAtt32"
}

layer{
  type: "Reshape"
  bottom: "obj32"
  top: "obj32_flat"
  reshape_param {
      shape {
        dim: 1  # copy the dimension from below
        dim: 1
        dim: 1
        dim: -1 # infer it from the other dimensions
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj32"
  top: "nonObj32_flat"
  reshape_param {
      shape {
        dim: 1  # copy the dimension from below
        dim: 1
        dim: 1
        dim: -1 # infer it from the other dimensions
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj32_flat"
  bottom: "nonObj32_flat"
  top: "obj32_checked"
  top: "nonObj32_checked"
}

######### attSize 64 ##########

layer {  name: "res_buAttSize1a_64"  type: "Convolution"  bottom: "res6g"  top: "res_buAttSize1a_64"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }      }
}

layer { name: "loss_buAttSize_prec_64" type: "InfogainLoss" bottom: "res_buAttSize1a_64" bottom: "scaleMask_64" propagate_down: true propagate_down: false top: "loss_attSize_prec_64" loss_weight: 1 infogain_loss_param{ source: "infogainH_64_2_800.binaryproto" }}

layer { name: "loss_buAttSize_64" type: "Softmax" bottom: "res_buAttSize1a_64" top: "finalAtt64" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt64"
  top: "nonObj64"
  top: "obj64"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_64"
#  top: "obj64"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#bias_term: true
#    bias_filler {
#      value: 1
#    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_64"
#  top: "nonObj64"
#  param {
#    lr_mult: 0
#    decay_mult: 0
# }
# scale_param {
#   filler {
#     value: 0    }
# }
#

#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_64"
#  top: "obj64"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 1    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_64"
#  top: "scaleMask_64_neg"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: -1    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_64"
#  top: "scaleMask_64_1"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#bias_term: true
#    bias_filler {
#      value: 1
#    }
#  }
#}
#layer { name: "el16" type: "Eltwise" bottom: "scaleMask_64_1" bottom: "scaleMask_64_neg" top: "nonObj64" }

layer{
  type: "Silence"
#  bottom: "masks"
#  bottom: "bbs"
#  bottom: "bbs_hw"
#  bottom: "options"
#  bottom: "configs"
  bottom: "nonObj64"
#  bottom: "finalAtt64"
}

layer{
  type: "Reshape"
  bottom: "obj64"
  top: "obj64_flat"
  reshape_param {
      shape {
        dim: 1  # copy the dimension from below
        dim: 1
        dim: 1
        dim: -1 # infer it from the other dimensions
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj64"
  top: "nonObj64_flat"
  reshape_param {
      shape {
        dim: 1  # copy the dimension from below
        dim: 1
        dim: 1
        dim: -1 # infer it from the other dimensions
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj64_flat"
  bottom: "nonObj64_flat"
  top: "obj64_checked"
  top: "nonObj64_checked"
}



######### attSize 128 ##########

layer {  name: "res_buAttSize1a_128"  type: "Convolution"  bottom: "res7g"  top: "res_buAttSize1a_128"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }      }
}

layer { name: "loss_buAttSize_prec_128" type: "InfogainLoss" bottom: "res_buAttSize1a_128" bottom: "scaleMask_128" propagate_down: true propagate_down: false top: "loss_attSize_prec_128" loss_weight: 1 infogain_loss_param{ source: "infogainH_128_2_800.binaryproto" }}

layer { name: "loss_buAttSize_128" type: "Softmax" bottom: "res_buAttSize1a_128" top: "finalAtt128" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt128"
  top: "nonObj128"
  top: "obj128"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_128"
#  top: "obj128"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#bias_term: true
#    bias_filler {
#      value: 1
#    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_128"
#  top: "nonObj128"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#  }
#}

#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_128"
#  top: "obj128"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 1    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_128"
#  top: "scaleMask_128_neg"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: -1    }
#  }
#}
#
#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "scaleMask_128"
#  top: "scaleMask_128_1"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 0    }
#bias_term: true
#    bias_filler {
#      value: 1
#    }
#  }
#}
#layer { name: "el16" type: "Eltwise" bottom: "scaleMask_128_1" bottom: "scaleMask_128_neg" top: "nonObj128" }

layer{
  type: "Silence"
#  bottom: "masks"
#  bottom: "bbs"
#  bottom: "bbs_hw"
#  bottom: "options"
#  bottom: "configs"
  bottom: "nonObj128"
#  bottom: "finalAtt128"
}

layer{
  type: "Reshape"
  bottom: "obj128"
  top: "obj128_flat"
  reshape_param {
      shape {
        dim: 1  # copy the dimension from below
        dim: 1
        dim: 1
        dim: -1 # infer it from the other dimensions
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj128"
  top: "nonObj128_flat"
  reshape_param {
      shape {
        dim: 1  # copy the dimension from below
        dim: 1
        dim: 1
        dim: -1 # infer it from the other dimensions
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj128_flat"
  bottom: "nonObj128_flat"
  top: "obj128_checked"
  top: "nonObj128_checked"
}




### BoxSelection #####################################

layer {
  name: "concatFlattendObj"
  bottom: "obj16_checked"
  bottom: "obj32_checked"
  bottom: "obj64_checked"
  bottom: "obj128_checked"
  top: "flattenedObj"
  type: "Concat"
  concat_param {
    axis: -1
  }
}

layer {
  name: "concatFlattendNonObj"
  bottom: "nonObj16_checked"
  bottom: "nonObj32_checked"
  bottom: "nonObj64_checked"
  bottom: "nonObj128_checked"
  top: "flattenedNonObj"
  type: "Concat"
  concat_param {
    axis: -1
  }
}

layer {
  name:"argmax"
  type: "ArgMaxMy"
  bottom: "flattenedObj"
  bottom: "flattenedNonObj"
  top: "obj_argmax"
  argmax_param {
#    top_k: 200
    axis: -1
  }
}

layer {
  name:"flags"
  type: "ConvertIndices"
  bottom: "obj_argmax"
  bottom: "flattenedObj"
  top: "obj_flags"
  top: "objn_new"

propagate_down: false
  propagate_down: false
}

#layer {
#  name: "flagsToIndices"
#  type: "FlagsToIndices"
#  bottom: "obj_flags"
#  top: "obj_indices"
#}

layer {
  name: "boxSelection"
  type: "Python"
  bottom: "obj_flags"
#  bottom: "masks"
#  bottom: "bbs"
#  bottom: "bbs_hw"
#  bottom: "options"
#  bottom: "configs"
  bottom: "id"
#  bottom: "data"
#  bottom: "scaleMask_16"
#  bottom: "scaleMask_32"
#  bottom: "scaleMask_64"
#  bottom: "scaleMask_128"
#  bottom: "obj16"
#  bottom: "obj32"
#  bottom: "obj64"
#  bottom: "obj128"
  top: "gt_objn"
  top: "gt_masks"
  top: "gt_atts"
  top: "objn_filter"
  top: "mask_filter"
#  top: "newFlags"
  python_param {
    module: "data.boxSelectionLayerMP2"
    layer: "BoxSelectionLayer"
  }
  propagate_down: false
  propagate_down: false
#  propagate_down: false
#  propagate_down: false
#  propagate_down: false
#  propagate_down: false
#  propagate_down: false
}


#layer {
#  name: "boxSelection"
#  type: "Python"
#  bottom: "objn_filter"
#  bottom: "objn_filter_old"
#  python_param {
#    module: "data.printLayer"
#    layer: "PrintLayer"
#  }
#}


#####split up indices######
layer{
  type: "SplitIndices"
  bottom: "obj_flags"
  bottom: "obj16_checked"
  bottom: "obj32_checked"
  bottom: "obj64_checked"
  bottom: "obj128_checked"
  top: "obj16_flags"
  top: "obj32_flags"
  top: "obj64_flags"
  top: "obj128_flags"
}

#layer {
#  name: "scaleAndAdd"
#  type: "Scale"
#  bottom: "obj_flags"
#  top: "scaled"
#  param {
#    lr_mult: 0
#    decay_mult: 0
#  }
#  scale_param {
#    filler {
#      value: 1    }
#  }
#}

##### sample windows######

layer { name: "conv_feat_1_16s" type: "Convolution" bottom: "res4g" top: "conv_feat_1_16s"
  param { name: "conv_feat_1" lr_mult: 10 } 
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false }     }

layer { name: "extractor_16" type: "SlidingWindowIndex" bottom: "conv_feat_1_16s" bottom: "obj16_flags" propagate_down: true propagate_down: false top: "sample_16s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_32s" type: "Convolution" bottom: "res5g" top: "conv_feat_1_32s"
  param { name: "conv_feat_1" lr_mult: 10 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false }     }

layer { name: "extractor_32" type: "SlidingWindowIndex" bottom: "conv_feat_1_32s" bottom: "obj32_flags" propagate_down: true propagate_down: false top: "sample_32s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_64s" type: "Convolution" bottom: "res6g" top: "conv_feat_1_64s"
  param { name: "conv_feat_1" lr_mult: 10 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

layer { name: "extractor_64" type: "SlidingWindowIndex" bottom: "conv_feat_1_64s" bottom: "obj64_flags" propagate_down: true propagate_down: false top: "sample_64s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_128s" type: "Convolution" bottom: "res7g" top: "conv_feat_1_128s"
  param { name: "conv_feat_1" lr_mult: 10 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false }     }

layer { name: "extractor_128" type: "SlidingWindowIndex" bottom: "conv_feat_1_128s" bottom: "obj128_flags" propagate_down: true propagate_down: false top: "sample_128s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "sample_concat" type: "Concat"
   bottom: "sample_16s"  bottom: "sample_32s"
  bottom: "sample_64s" bottom: "sample_128s" 
top: "sample"
  concat_param { concat_dim: 0 } }

layer { bottom: "sample" top: "sample" type: "BatchNorm" name:"sample_bn" batch_norm_param { use_global_stats: false } 
  param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } }
layer { bottom: "sample" top: "sample" type: "Scale" name: "sample_scale" scale_param { bias_term: true } 
  param { lr_mult: 10 decay_mult: 0 } param { lr_mult: 20 decay_mult: 0} }

layer { name: "batch_filter" type: "TopK" bottom: "sample" bottom: "mask_filter" bottom: "gt_masks" top: "filted_sample" }

#layer {
#  name: "boxSelection"
#  type: "Python"
#  bottom: "filted_sample"
#  top: "filted_sample_a"
#  python_param {
#    module: "data.printReturnLayer"
#    layer: "PrintReturnLayer"
#  }
#}

########## cls branch ##########

layer { name: 'cls_1' type: 'InnerProduct' bottom: 'sample' top: 'cls_1'
    param { lr_mult: 10.0 } param { lr_mult: 20.0 }
    inner_product_param { num_output: 512 weight_filler: { type: 'gaussian' std: 0.01 } } }
layer { name: 'relu_cls_1' type: 'ReLU' bottom: 'cls_1' top: 'cls_1'     }
layer { name: 'dropout_cls_1' type: 'Dropout' bottom: 'cls_1' top: 'cls_1' dropout_param { dropout_ratio: 0.5 } }

layer { name: 'cls_2' type: 'InnerProduct' bottom: 'cls_1' top: 'cls_2'
  param { lr_mult: 10.0 } param { lr_mult: 20.0 }
  inner_product_param { num_output: 1024 weight_filler: { type: 'gaussian' std: 0.01 } } }
layer { name: 'relu_cls_2' type: 'ReLU' bottom: 'cls_2' top: 'cls_2'     }

########## cls score ##########

layer { name: 'obj_score' type: 'InnerProduct' bottom: 'cls_2' top: 'obj_score' 
  param { lr_mult: 10.0 } param { lr_mult: 20.0 }
  inner_product_param { num_output: 1 weight_filler { type: "gaussian" std: 0.001 } bias_filler { type: "constant" std: 0 } } }
layer { name: "obj_reshape" type: "Reshape" bottom: "obj_score" top: "obj_score_reshape" 
  reshape_param { shape { dim: -1 dim: 1 dim: 1 dim: 1 } } }

#layer { name: "objn_filter" type: "TopK" bottom: "obj_score_reshape" bottom: "cls_reshape" bottom: "cls2" top: "objn_filted" }
#
#layer { name: 'loss_obj' type: 'SigmoidCrossEntropyLoss' bottom: 'objn_filted' bottom: "cls2" propagate_down: true propagate_down: false top: 'loss_obj' loss_weight: 1}

layer { name: "objn_filter" type: "TopK" bottom: "obj_score_reshape" bottom: "objn_filter" bottom: "gt_objn" top: "objn_filted" }

#layer {
#  name: "boxSelection"
#  type: "Python"
#  bottom: "obj_score_reshape"
#  bottom: "objn_filter"
#  bottom: "mask_filter"
#  bottom: "gt_objn"
#  bottom: "objn_filted" 
## bottom: "newFlags"
# bottom: "sample"
#  python_param {
#    module: "data.printLayer"
#    layer: "PrintLayer"
#  }
#}

layer { name: 'loss_obj' type: 'SigmoidCrossEntropyLoss' bottom: 'objn_filted' bottom: "gt_objn" propagate_down: true propagate_down: false top: 'loss_obj' loss_weight: 1}

layer{
  type: "Silence"
#  bottom: "gt_objns_old"
#  bottom: "gt_masks_old"
#  bottom: "gt_atts_old"
#  bottom: "objn_filter_old"
#  bottom: "mask_filter_old"
  bottom: "obj32_flags"
  bottom: "obj64_flags"
  bottom: "obj128_flags"
  bottom: "sample_16s"
  bottom: "sample_32s"
  bottom: "sample_64s"
  bottom: "sample_128s"
  bottom: "sample"
  bottom: "filted_sample"
  bottom: "objn_new"
}

########## att branch ##########
layer { name: 'att' type: 'InnerProduct' bottom: 'filted_sample' top: 'flatten_att'
  param { lr_mult: 10.0 } param { lr_mult: 20.0 }
  inner_product_param { num_output: 100 weight_filler: { type: 'gaussian' std: 0.0001 } bias_filler { type: 'constant' value: 0 } } }

layer {
  name: "atts_reshape" type: "Reshape" bottom: "flatten_att" top: "atts"
  reshape_param { shape { dim: -1 dim: 1 dim: 10, dim: 10 } }
}


########## att score ###########

layer { name: "loss_att" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "atts" bottom: "gt_atts" propagate_down: true propagate_down: false top: "loss_atts" loss_weight: 1}
#layer { name: "loss_att" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "atts" bottom: "att" propagate_down: true propagate_down: false top: "loss_atts" loss_weight: 1}
######### att filter ###########
layer { name: 'att_sigmoid' type: 'Sigmoid' bottom: 'atts' top: 'sig_atts' }
layer { name: 'att_filt' type: 'TileProduct' bottom: 'filted_sample' bottom: 'sig_atts' top: 'atts_filt_feat' }

########## seg branch ##########

layer { name: 'seg_1' type: 'InnerProduct' bottom: 'atts_filt_feat' top: 'seg_1'
    param { lr_mult: 10.0 }
    inner_product_param { num_output: 512 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } } 

layer { bottom: "seg_1" top: "seg_1" type: "BatchNorm" name:"seg_1_bn" batch_norm_param { use_global_stats: true } 
  param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } }
layer { bottom: "seg_1" top: "seg_1" type: "Scale" name: "seg_1_scale" scale_param { bias_term: true } 
  param { lr_mult: 10 decay_mult: 0 } param { lr_mult: 20 decay_mult: 0} }

layer { name: 'seg_2' type: 'InnerProduct' bottom: 'seg_1' top: 'seg_2'
  param { lr_mult: 10.0 } param { lr_mult: 20.0 }
  inner_product_param { num_output: 1600 weight_filler: { type: 'gaussian' std: 0.001 } bias_filler { type: 'constant' value: 0 } } }

########## seg score ##########
layer { name: 'mask_reshape' type: 'Reshape' bottom: 'seg_2' top: 'seg_mask'
  reshape_param { shape { dim: -1 dim: 1 dim: 40 dim: 40 } } }

layer { name: 'upscore' type: "Deconvolution" bottom: "seg_mask" top: "seg_mask_up" param { lr_mult: 0 }
  convolution_param { num_output: 1 bias_term: false kernel_size: 8 stride: 4 } }

#layer { name: "reshape" type: "Reshape" bottom: "seg" top: "gt_masks_reshape" reshape_param { shape { dim: -1 dim: 1 dim: 80 dim: 80 } } }
layer { name: "reshape" type: "Reshape" bottom: "gt_masks" top: "gt_masks_reshape" reshape_param { shape { dim: -1 dim: 1 dim: 160 dim: 160 } } }

layer { name: "crop" type: "Crop" bottom: "seg_mask_up" bottom: "gt_masks_reshape" top: "seg_mask_crop" crop_param { axis: 2 offset: 2 } }

layer { name: 'loss_seg' type: 'NormalizedSigmoidCrossEntropyLoss' bottom: 'seg_mask_crop' bottom: 'gt_masks_reshape' propagate_down: true propagate_down: false top: 'loss_mask' loss_weight: 1}
