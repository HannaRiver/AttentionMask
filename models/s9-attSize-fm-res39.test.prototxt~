name: 'Single Shot Mask - ResNet 50'

layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 224
      dim: 224
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "bn_conv1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale_conv1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_relu"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "res2a_branch1"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch1"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn2a_branch1"
  type: "BatchNorm"
  bottom: "res2a_branch1"
  top: "res2a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2a_branch1"
  type: "Scale"
  bottom: "res2a_branch1"
  top: "res2a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn2a_branch2a"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2a_branch2a"
  type: "Scale"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2a_branch2a_relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn2a_branch2b"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2a_branch2b"
  type: "Scale"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2a_branch2b_relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "res2a_branch2c"
  type: "Convolution"
  bottom: "res2a_branch2b"
  top: "res2a_branch2c"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn2a_branch2c"
  type: "BatchNorm"
  bottom: "res2a_branch2c"
  top: "res2a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2a_branch2c"
  type: "Scale"
  bottom: "res2a_branch2c"
  top: "res2a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2a"
  type: "Eltwise"
  bottom: "res2a_branch1"
  bottom: "res2a_branch2c"
  top: "res2a"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2a_relu"
  type: "ReLU"
  bottom: "res2a"
  top: "res2a"
}
layer {
  name: "res2b_branch2a"
  type: "Convolution"
  bottom: "res2a"
  top: "res2b_branch2a"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn2b_branch2a"
  type: "BatchNorm"
  bottom: "res2b_branch2a"
  top: "res2b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2b_branch2a"
  type: "Scale"
  bottom: "res2b_branch2a"
  top: "res2b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2b_branch2a_relu"
  type: "ReLU"
  bottom: "res2b_branch2a"
  top: "res2b_branch2a"
}
layer {
  name: "res2b_branch2b"
  type: "Convolution"
  bottom: "res2b_branch2a"
  top: "res2b_branch2b"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn2b_branch2b"
  type: "BatchNorm"
  bottom: "res2b_branch2b"
  top: "res2b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2b_branch2b"
  type: "Scale"
  bottom: "res2b_branch2b"
  top: "res2b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2b_branch2b_relu"
  type: "ReLU"
  bottom: "res2b_branch2b"
  top: "res2b_branch2b"
}
layer {
  name: "res2b_branch2c"
  type: "Convolution"
  bottom: "res2b_branch2b"
  top: "res2b_branch2c"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn2b_branch2c"
  type: "BatchNorm"
  bottom: "res2b_branch2c"
  top: "res2b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2b_branch2c"
  type: "Scale"
  bottom: "res2b_branch2c"
  top: "res2b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2b"
  type: "Eltwise"
  bottom: "res2a"
  bottom: "res2b_branch2c"
  top: "res2b"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2b_relu"
  type: "ReLU"
  bottom: "res2b"
  top: "res2b"
}
layer {
  name: "res2c_branch2a"
  type: "Convolution"
  bottom: "res2b"
  top: "res2c_branch2a"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn2c_branch2a"
  type: "BatchNorm"
  bottom: "res2c_branch2a"
  top: "res2c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2c_branch2a"
  type: "Scale"
  bottom: "res2c_branch2a"
  top: "res2c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2c_branch2a_relu"
  type: "ReLU"
  bottom: "res2c_branch2a"
  top: "res2c_branch2a"
}
layer {
  name: "res2c_branch2b"
  type: "Convolution"
  bottom: "res2c_branch2a"
  top: "res2c_branch2b"
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn2c_branch2b"
  type: "BatchNorm"
  bottom: "res2c_branch2b"
  top: "res2c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2c_branch2b"
  type: "Scale"
  bottom: "res2c_branch2b"
  top: "res2c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2c_branch2b_relu"
  type: "ReLU"
  bottom: "res2c_branch2b"
  top: "res2c_branch2b"
}
layer {
  name: "res2c_branch2c"
  type: "Convolution"
  bottom: "res2c_branch2b"
  top: "res2c_branch2c"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn2c_branch2c"
  type: "BatchNorm"
  bottom: "res2c_branch2c"
  top: "res2c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale2c_branch2c"
  type: "Scale"
  bottom: "res2c_branch2c"
  top: "res2c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res2c"
  type: "Eltwise"
  bottom: "res2b"
  bottom: "res2c_branch2c"
  top: "res2c"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res2c_relu"
  type: "ReLU"
  bottom: "res2c"
  top: "res2c"
}
layer {
  name: "res3a_branch1"
  type: "Convolution"
  bottom: "res2c"
  top: "res3a_branch1"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
  }
}
layer {
  name: "bn3a_branch1"
  type: "BatchNorm"
  bottom: "res3a_branch1"
  top: "res3a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3a_branch1"
  type: "Scale"
  bottom: "res3a_branch1"
  top: "res3a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "res2c"
  top: "res3a_branch2a"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
  }
}
layer {
  name: "bn3a_branch2a"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3a_branch2a"
  type: "Scale"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3a_branch2a_relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn3a_branch2b"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3a_branch2b"
  type: "Scale"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3a_branch2b_relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "res3a_branch2c"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "res3a_branch2c"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn3a_branch2c"
  type: "BatchNorm"
  bottom: "res3a_branch2c"
  top: "res3a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3a_branch2c"
  type: "Scale"
  bottom: "res3a_branch2c"
  top: "res3a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3a"
  type: "Eltwise"
  bottom: "res3a_branch1"
  bottom: "res3a_branch2c"
  top: "res3a"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3a_relu"
  type: "ReLU"
  bottom: "res3a"
  top: "res3a"
}
layer {
  name: "res3b_branch2a"
  type: "Convolution"
  bottom: "res3a"
  top: "res3b_branch2a"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn3b_branch2a"
  type: "BatchNorm"
  bottom: "res3b_branch2a"
  top: "res3b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3b_branch2a"
  type: "Scale"
  bottom: "res3b_branch2a"
  top: "res3b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3b_branch2a_relu"
  type: "ReLU"
  bottom: "res3b_branch2a"
  top: "res3b_branch2a"
}
layer {
  name: "res3b_branch2b"
  type: "Convolution"
  bottom: "res3b_branch2a"
  top: "res3b_branch2b"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn3b_branch2b"
  type: "BatchNorm"
  bottom: "res3b_branch2b"
  top: "res3b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3b_branch2b"
  type: "Scale"
  bottom: "res3b_branch2b"
  top: "res3b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3b_branch2b_relu"
  type: "ReLU"
  bottom: "res3b_branch2b"
  top: "res3b_branch2b"
}
layer {
  name: "res3b_branch2c"
  type: "Convolution"
  bottom: "res3b_branch2b"
  top: "res3b_branch2c"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn3b_branch2c"
  type: "BatchNorm"
  bottom: "res3b_branch2c"
  top: "res3b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3b_branch2c"
  type: "Scale"
  bottom: "res3b_branch2c"
  top: "res3b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3b"
  type: "Eltwise"
  bottom: "res3a"
  bottom: "res3b_branch2c"
  top: "res3b"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3b_relu"
  type: "ReLU"
  bottom: "res3b"
  top: "res3b"
}
layer {
  name: "res3c_branch2a"
  type: "Convolution"
  bottom: "res3b"
  top: "res3c_branch2a"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn3c_branch2a"
  type: "BatchNorm"
  bottom: "res3c_branch2a"
  top: "res3c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3c_branch2a"
  type: "Scale"
  bottom: "res3c_branch2a"
  top: "res3c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3c_branch2a_relu"
  type: "ReLU"
  bottom: "res3c_branch2a"
  top: "res3c_branch2a"
}
layer {
  name: "res3c_branch2b"
  type: "Convolution"
  bottom: "res3c_branch2a"
  top: "res3c_branch2b"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn3c_branch2b"
  type: "BatchNorm"
  bottom: "res3c_branch2b"
  top: "res3c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3c_branch2b"
  type: "Scale"
  bottom: "res3c_branch2b"
  top: "res3c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3c_branch2b_relu"
  type: "ReLU"
  bottom: "res3c_branch2b"
  top: "res3c_branch2b"
}
layer {
  name: "res3c_branch2c"
  type: "Convolution"
  bottom: "res3c_branch2b"
  top: "res3c_branch2c"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn3c_branch2c"
  type: "BatchNorm"
  bottom: "res3c_branch2c"
  top: "res3c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3c_branch2c"
  type: "Scale"
  bottom: "res3c_branch2c"
  top: "res3c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3c"
  type: "Eltwise"
  bottom: "res3b"
  bottom: "res3c_branch2c"
  top: "res3c"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3c_relu"
  type: "ReLU"
  bottom: "res3c"
  top: "res3c"
}
layer {
  name: "res3d_branch2a"
  type: "Convolution"
  bottom: "res3c"
  top: "res3d_branch2a"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn3d_branch2a"
  type: "BatchNorm"
  bottom: "res3d_branch2a"
  top: "res3d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3d_branch2a"
  type: "Scale"
  bottom: "res3d_branch2a"
  top: "res3d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3d_branch2a_relu"
  type: "ReLU"
  bottom: "res3d_branch2a"
  top: "res3d_branch2a"
}
layer {
  name: "res3d_branch2b"
  type: "Convolution"
  bottom: "res3d_branch2a"
  top: "res3d_branch2b"
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn3d_branch2b"
  type: "BatchNorm"
  bottom: "res3d_branch2b"
  top: "res3d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3d_branch2b"
  type: "Scale"
  bottom: "res3d_branch2b"
  top: "res3d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3d_branch2b_relu"
  type: "ReLU"
  bottom: "res3d_branch2b"
  top: "res3d_branch2b"
}
layer {
  name: "res3d_branch2c"
  type: "Convolution"
  bottom: "res3d_branch2b"
  top: "res3d_branch2c"
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn3d_branch2c"
  type: "BatchNorm"
  bottom: "res3d_branch2c"
  top: "res3d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale3d_branch2c"
  type: "Scale"
  bottom: "res3d_branch2c"
  top: "res3d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res3d"
  type: "Eltwise"
  bottom: "res3c"
  bottom: "res3d_branch2c"
  top: "res3d"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res3d_relu"
  type: "ReLU"
  bottom: "res3d"
  top: "res3d"
}
layer {
  name: "res4a_branch1"
  type: "Convolution"
  bottom: "res3d"
  top: "res4a_branch1"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
  }
}
layer {
  name: "bn4a_branch1"
  type: "BatchNorm"
  bottom: "res4a_branch1"
  top: "res4a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch1"
  type: "Scale"
  bottom: "res4a_branch1"
  top: "res4a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "res3d"
  top: "res4a_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
  }
}
layer {
  name: "bn4a_branch2a"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch2a"
  type: "Scale"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_branch2a_relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4a_branch2b"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch2b"
  type: "Scale"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_branch2b_relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "res4a_branch2c"
  type: "Convolution"
  bottom: "res4a_branch2b"
  top: "res4a_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4a_branch2c"
  type: "BatchNorm"
  bottom: "res4a_branch2c"
  top: "res4a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch2c"
  type: "Scale"
  bottom: "res4a_branch2c"
  top: "res4a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a"
  type: "Eltwise"
  bottom: "res4a_branch1"
  bottom: "res4a_branch2c"
  top: "res4a"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4a_relu"
  type: "ReLU"
  bottom: "res4a"
  top: "res4a"
}
layer {
  name: "res4b_branch2a"
  type: "Convolution"
  bottom: "res4a"
  top: "res4b_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4b_branch2a"
  type: "BatchNorm"
  bottom: "res4b_branch2a"
  top: "res4b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4b_branch2a"
  type: "Scale"
  bottom: "res4b_branch2a"
  top: "res4b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4b_branch2a_relu"
  type: "ReLU"
  bottom: "res4b_branch2a"
  top: "res4b_branch2a"
}
layer {
  name: "res4b_branch2b"
  type: "Convolution"
  bottom: "res4b_branch2a"
  top: "res4b_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4b_branch2b"
  type: "BatchNorm"
  bottom: "res4b_branch2b"
  top: "res4b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4b_branch2b"
  type: "Scale"
  bottom: "res4b_branch2b"
  top: "res4b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4b_branch2b_relu"
  type: "ReLU"
  bottom: "res4b_branch2b"
  top: "res4b_branch2b"
}
layer {
  name: "res4b_branch2c"
  type: "Convolution"
  bottom: "res4b_branch2b"
  top: "res4b_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4b_branch2c"
  type: "BatchNorm"
  bottom: "res4b_branch2c"
  top: "res4b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4b_branch2c"
  type: "Scale"
  bottom: "res4b_branch2c"
  top: "res4b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4b"
  type: "Eltwise"
  bottom: "res4a"
  bottom: "res4b_branch2c"
  top: "res4b"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4b_relu"
  type: "ReLU"
  bottom: "res4b"
  top: "res4b"
}
layer {
  name: "res4c_branch2a"
  type: "Convolution"
  bottom: "res4b"
  top: "res4c_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4c_branch2a"
  type: "BatchNorm"
  bottom: "res4c_branch2a"
  top: "res4c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4c_branch2a"
  type: "Scale"
  bottom: "res4c_branch2a"
  top: "res4c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4c_branch2a_relu"
  type: "ReLU"
  bottom: "res4c_branch2a"
  top: "res4c_branch2a"
}
layer {
  name: "res4c_branch2b"
  type: "Convolution"
  bottom: "res4c_branch2a"
  top: "res4c_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4c_branch2b"
  type: "BatchNorm"
  bottom: "res4c_branch2b"
  top: "res4c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4c_branch2b"
  type: "Scale"
  bottom: "res4c_branch2b"
  top: "res4c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4c_branch2b_relu"
  type: "ReLU"
  bottom: "res4c_branch2b"
  top: "res4c_branch2b"
}
layer {
  name: "res4c_branch2c"
  type: "Convolution"
  bottom: "res4c_branch2b"
  top: "res4c_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4c_branch2c"
  type: "BatchNorm"
  bottom: "res4c_branch2c"
  top: "res4c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4c_branch2c"
  type: "Scale"
  bottom: "res4c_branch2c"
  top: "res4c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4c"
  type: "Eltwise"
  bottom: "res4b"
  bottom: "res4c_branch2c"
  top: "res4c"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4c_relu"
  type: "ReLU"
  bottom: "res4c"
  top: "res4c"
}
layer {
  name: "res4d_branch2a"
  type: "Convolution"
  bottom: "res4c"
  top: "res4d_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4d_branch2a"
  type: "BatchNorm"
  bottom: "res4d_branch2a"
  top: "res4d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4d_branch2a"
  type: "Scale"
  bottom: "res4d_branch2a"
  top: "res4d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4d_branch2a_relu"
  type: "ReLU"
  bottom: "res4d_branch2a"
  top: "res4d_branch2a"
}
layer {
  name: "res4d_branch2b"
  type: "Convolution"
  bottom: "res4d_branch2a"
  top: "res4d_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4d_branch2b"
  type: "BatchNorm"
  bottom: "res4d_branch2b"
  top: "res4d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4d_branch2b"
  type: "Scale"
  bottom: "res4d_branch2b"
  top: "res4d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4d_branch2b_relu"
  type: "ReLU"
  bottom: "res4d_branch2b"
  top: "res4d_branch2b"
}
layer {
  name: "res4d_branch2c"
  type: "Convolution"
  bottom: "res4d_branch2b"
  top: "res4d_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4d_branch2c"
  type: "BatchNorm"
  bottom: "res4d_branch2c"
  top: "res4d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4d_branch2c"
  type: "Scale"
  bottom: "res4d_branch2c"
  top: "res4d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4d"
  type: "Eltwise"
  bottom: "res4c"
  bottom: "res4d_branch2c"
  top: "res4d"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4d_relu"
  type: "ReLU"
  bottom: "res4d"
  top: "res4d"
}
layer {
  name: "res4e_branch2a"
  type: "Convolution"
  bottom: "res4d"
  top: "res4e_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4e_branch2a"
  type: "BatchNorm"
  bottom: "res4e_branch2a"
  top: "res4e_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4e_branch2a"
  type: "Scale"
  bottom: "res4e_branch2a"
  top: "res4e_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4e_branch2a_relu"
  type: "ReLU"
  bottom: "res4e_branch2a"
  top: "res4e_branch2a"
}
layer {
  name: "res4e_branch2b"
  type: "Convolution"
  bottom: "res4e_branch2a"
  top: "res4e_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4e_branch2b"
  type: "BatchNorm"
  bottom: "res4e_branch2b"
  top: "res4e_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4e_branch2b"
  type: "Scale"
  bottom: "res4e_branch2b"
  top: "res4e_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4e_branch2b_relu"
  type: "ReLU"
  bottom: "res4e_branch2b"
  top: "res4e_branch2b"
}
layer {
  name: "res4e_branch2c"
  type: "Convolution"
  bottom: "res4e_branch2b"
  top: "res4e_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4e_branch2c"
  type: "BatchNorm"
  bottom: "res4e_branch2c"
  top: "res4e_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4e_branch2c"
  type: "Scale"
  bottom: "res4e_branch2c"
  top: "res4e_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4e"
  type: "Eltwise"
  bottom: "res4d"
  bottom: "res4e_branch2c"
  top: "res4e"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4e_relu"
  type: "ReLU"
  bottom: "res4e"
  top: "res4e"
}
layer {
  name: "res4f_branch2a"
  type: "Convolution"
  bottom: "res4e"
  top: "res4f_branch2a"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4f_branch2a"
  type: "BatchNorm"
  bottom: "res4f_branch2a"
  top: "res4f_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4f_branch2a"
  type: "Scale"
  bottom: "res4f_branch2a"
  top: "res4f_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4f_branch2a_relu"
  type: "ReLU"
  bottom: "res4f_branch2a"
  top: "res4f_branch2a"
}
layer {
  name: "res4f_branch2b"
  type: "Convolution"
  bottom: "res4f_branch2a"
  top: "res4f_branch2b"
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4f_branch2b"
  type: "BatchNorm"
  bottom: "res4f_branch2b"
  top: "res4f_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4f_branch2b"
  type: "Scale"
  bottom: "res4f_branch2b"
  top: "res4f_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4f_branch2b_relu"
  type: "ReLU"
  bottom: "res4f_branch2b"
  top: "res4f_branch2b"
}
layer {
  name: "res4f_branch2c"
  type: "Convolution"
  bottom: "res4f_branch2b"
  top: "res4f_branch2c"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4f_branch2c"
  type: "BatchNorm"
  bottom: "res4f_branch2c"
  top: "res4f_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4f_branch2c"
  type: "Scale"
  bottom: "res4f_branch2c"
  top: "res4f_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4f"
  type: "Eltwise"
  bottom: "res4e"
  bottom: "res4f_branch2c"
  top: "res4f"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4f_relu"
  type: "ReLU"
  bottom: "res4f"
  top: "res4f"
}

########## div 3 branch #############
layer {
  name: "res4a_branch1_div3"
  type: "Convolution"
  bottom: "res3d"
  top: "res4a_branch1_div3"
  param {
    name: "res4a_branch1_weight"
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 3
  }
}
layer {
  name: "bn4a_branch1_div3"
  type: "BatchNorm"
  bottom: "res4a_branch1_div3"
  top: "res4a_branch1_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch1_div3"
  type: "Scale"
  bottom: "res4a_branch1_div3"
  top: "res4a_branch1_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_branch2a_div3"
  type: "Convolution"
  bottom: "res3d"
  top: "res4a_branch2a_div3"
  param {
    name: "res4a_branch2a_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 3
  }
}
layer {
  name: "bn4a_branch2a_div3"
  type: "BatchNorm"
  bottom: "res4a_branch2a_div3"
  top: "res4a_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch2a_div3"
  type: "Scale"
  bottom: "res4a_branch2a_div3"
  top: "res4a_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_branch2a_relu_div3"
  type: "ReLU"
  bottom: "res4a_branch2a_div3"
  top: "res4a_branch2a_div3"
}
layer {
  name: "res4a_branch2b_div3"
  type: "Convolution"
  bottom: "res4a_branch2a_div3"
  top: "res4a_branch2b_div3"
  param {
    name: "res4a_branch2b_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4a_branch2b_div3"
  type: "BatchNorm"
  bottom: "res4a_branch2b_div3"
  top: "res4a_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch2b_div3"
  type: "Scale"
  bottom: "res4a_branch2b_div3"
  top: "res4a_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_branch2b_relu_div3"
  type: "ReLU"
  bottom: "res4a_branch2b_div3"
  top: "res4a_branch2b_div3"
}
layer {
  name: "res4a_branch2c_div3"
  type: "Convolution"
  param {
    name: "res4a_branch2c_weight"
  }
  bottom: "res4a_branch2b_div3"
  top: "res4a_branch2c_div3"
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4a_branch2c_div3"
  type: "BatchNorm"
  bottom: "res4a_branch2c_div3"
  top: "res4a_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4a_branch2c_div3"
  type: "Scale"
  bottom: "res4a_branch2c_div3"
  top: "res4a_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4a_div3"
  type: "Eltwise"
  bottom: "res4a_branch1_div3"
  bottom: "res4a_branch2c_div3"
  top: "res4a_div3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4a_relu_div3"
  type: "ReLU"
  bottom: "res4a_div3"
  top: "res4a_div3"
}
layer {
  name: "res4b_branch2a_div3"
  type: "Convolution"
  bottom: "res4a_div3"
  top: "res4b_branch2a_div3"
  param {
    name: "res4b_branch2a_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4b_branch2a_div3"
  type: "BatchNorm"
  bottom: "res4b_branch2a_div3"
  top: "res4b_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4b_branch2a_div3"
  type: "Scale"
  bottom: "res4b_branch2a_div3"
  top: "res4b_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4b_branch2a_relu_div3"
  type: "ReLU"
  bottom: "res4b_branch2a_div3"
  top: "res4b_branch2a_div3"
}
layer {
  name: "res4b_branch2b_div3"
  type: "Convolution"
  bottom: "res4b_branch2a_div3"
  top: "res4b_branch2b_div3"
  param {
    name: "res4b_branch2b_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4b_branch2b_div3"
  type: "BatchNorm"
  bottom: "res4b_branch2b_div3"
  top: "res4b_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4b_branch2b_div3"
  type: "Scale"
  bottom: "res4b_branch2b_div3"
  top: "res4b_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4b_branch2b_relu_div3"
  type: "ReLU"
  bottom: "res4b_branch2b_div3"
  top: "res4b_branch2b_div3"
}
layer {
  name: "res4b_branch2c_div3"
  type: "Convolution"
  bottom: "res4b_branch2b_div3"
  top: "res4b_branch2c_div3"
  param {
    name: "res4b_branch2c_weight"
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4b_branch2c_div3"
  type: "BatchNorm"
  bottom: "res4b_branch2c_div3"
  top: "res4b_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4b_branch2c_div3"
  type: "Scale"
  bottom: "res4b_branch2c_div3"
  top: "res4b_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4b"
  type: "Eltwise"
  bottom: "res4a_div3"
  bottom: "res4b_branch2c_div3"
  top: "res4b_div3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4b_relu_div3"
  type: "ReLU"
  bottom: "res4b_div3"
  top: "res4b_div3"
}
layer {
  name: "res4c_branch2a_div3"
  type: "Convolution"
  bottom: "res4b_div3"
  top: "res4c_branch2a_div3"
  param {
    name: "res4c_branch2a_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4c_branch2a_div3"
  type: "BatchNorm"
  bottom: "res4c_branch2a_div3"
  top: "res4c_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4c_branch2a_div3"
  type: "Scale"
  bottom: "res4c_branch2a_div3"
  top: "res4c_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4c_branch2a_relu_div3"
  type: "ReLU"
  bottom: "res4c_branch2a_div3"
  top: "res4c_branch2a_div3"
}
layer {
  name: "res4c_branch2b_div3"
  type: "Convolution"
  bottom: "res4c_branch2a_div3"
  top: "res4c_branch2b_div3"
  param {
    name: "res4c_branch2b_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4c_branch2b_div3"
  type: "BatchNorm"
  bottom: "res4c_branch2b_div3"
  top: "res4c_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4c_branch2b_div3"
  type: "Scale"
  bottom: "res4c_branch2b_div3"
  top: "res4c_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4c_branch2b_relu_div3"
  type: "ReLU"
  bottom: "res4c_branch2b_div3"
  top: "res4c_branch2b_div3"
}
layer {
  name: "res4c_branch2c_div3"
  type: "Convolution"
  bottom: "res4c_branch2b_div3"
  top: "res4c_branch2c_div3"
  param {
    name: "res4c_branch2c_weight"
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4c_branch2c_div3"
  type: "BatchNorm"
  bottom: "res4c_branch2c_div3"
  top: "res4c_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4c_branch2c_div3"
  type: "Scale"
  bottom: "res4c_branch2c_div3"
  top: "res4c_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4c_div3"
  type: "Eltwise"
  bottom: "res4b_div3"
  bottom: "res4c_branch2c_div3"
  top: "res4c_div3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4c_relu_div3"
  type: "ReLU"
  bottom: "res4c_div3"
  top: "res4c_div3"
}
layer {
  name: "res4d_branch2a_div3"
  type: "Convolution"
  bottom: "res4c_div3"
  top: "res4d_branch2a_div3"
  param {
    name: "res4d_branch2a_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4d_branch2a_div3"
  type: "BatchNorm"
  bottom: "res4d_branch2a_div3"
  top: "res4d_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4d_branch2a_div3"
  type: "Scale"
  bottom: "res4d_branch2a_div3"
  top: "res4d_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4d_branch2a_relu_div3"
  type: "ReLU"
  bottom: "res4d_branch2a_div3"
  top: "res4d_branch2a_div3"
}
layer {
  name: "res4d_branch2b_div3"
  type: "Convolution"
  bottom: "res4d_branch2a_div3"
  top: "res4d_branch2b_div3"
  param {
    name: "res4d_branch2b_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4d_branch2b_div3"
  type: "BatchNorm"
  bottom: "res4d_branch2b_div3"
  top: "res4d_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4d_branch2b_div3"
  type: "Scale"
  bottom: "res4d_branch2b_div3"
  top: "res4d_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4d_branch2b_relu_div3"
  type: "ReLU"
  bottom: "res4d_branch2b_div3"
  top: "res4d_branch2b_div3"
}
layer {
  name: "res4d_branch2c_div3"
  type: "Convolution"
  bottom: "res4d_branch2b_div3"
  top: "res4d_branch2c_div3"
  param {
    name: "res4d_branch2c_weight"
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4d_branch2c_div3"
  type: "BatchNorm"
  bottom: "res4d_branch2c_div3"
  top: "res4d_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4d_branch2c_div3"
  type: "Scale"
  bottom: "res4d_branch2c_div3"
  top: "res4d_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4d_div3"
  type: "Eltwise"
  bottom: "res4c_div3"
  bottom: "res4d_branch2c_div3"
  top: "res4d_div3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4d_relu_div3"
  type: "ReLU"
  bottom: "res4d_div3"
  top: "res4d_div3"
}
layer {
  name: "res4e_branch2a_div3"
  type: "Convolution"
  bottom: "res4d_div3"
  top: "res4e_branch2a_div3"
  param {
    name: "res4e_branch2a_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4e_branch2a_div3"
  type: "BatchNorm"
  bottom: "res4e_branch2a_div3"
  top: "res4e_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4e_branch2a_div3"
  type: "Scale"
  bottom: "res4e_branch2a_div3"
  top: "res4e_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4e_branch2a_relu_div3"
  type: "ReLU"
  bottom: "res4e_branch2a_div3"
  top: "res4e_branch2a_div3"
}
layer {
  name: "res4e_branch2b_div3"
  type: "Convolution"
  bottom: "res4e_branch2a_div3"
  top: "res4e_branch2b_div3"
  param {
    name: "res4e_branch2b_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4e_branch2b_div3"
  type: "BatchNorm"
  bottom: "res4e_branch2b_div3"
  top: "res4e_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4e_branch2b_div3"
  type: "Scale"
  bottom: "res4e_branch2b_div3"
  top: "res4e_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4e_branch2b_relu_div3"
  type: "ReLU"
  bottom: "res4e_branch2b_div3"
  top: "res4e_branch2b_div3"
}
layer {
  name: "res4e_branch2c_div3"
  type: "Convolution"
  bottom: "res4e_branch2b_div3"
  top: "res4e_branch2c_div3"
  param {
    name: "res4e_branch2c_weight"
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4e_branch2c_div3"
  type: "BatchNorm"
  bottom: "res4e_branch2c_div3"
  top: "res4e_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4e_branch2c_div3"
  type: "Scale"
  bottom: "res4e_branch2c_div3"
  top: "res4e_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4e_div3"
  type: "Eltwise"
  bottom: "res4d_div3"
  bottom: "res4e_branch2c_div3"
  top: "res4e_div3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4e_relu_div3"
  type: "ReLU"
  bottom: "res4e_div3"
  top: "res4e_div3"
}
layer {
  name: "res4f_branch2a_div3"
  type: "Convolution"
  bottom: "res4e_div3"
  top: "res4f_branch2a_div3"
  param {
    name: "res4f_branch2a_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4f_branch2a_div3"
  type: "BatchNorm"
  bottom: "res4f_branch2a_div3"
  top: "res4f_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4f_branch2a_div3"
  type: "Scale"
  bottom: "res4f_branch2a_div3"
  top: "res4f_branch2a_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4f_branch2a_relu_div3"
  type: "ReLU"
  bottom: "res4f_branch2a_div3"
  top: "res4f_branch2a_div3"
}
layer {
  name: "res4f_branch2b_div3"
  type: "Convolution"
  bottom: "res4f_branch2a_div3"
  top: "res4f_branch2b_div3"
  param {
    name: "res4f_branch2b_weight"
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "bn4f_branch2b_div3"
  type: "BatchNorm"
  bottom: "res4f_branch2b_div3"
  top: "res4f_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4f_branch2b_div3"
  type: "Scale"
  bottom: "res4f_branch2b_div3"
  top: "res4f_branch2b_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4f_branch2b_relu_div3"
  type: "ReLU"
  bottom: "res4f_branch2b_div3"
  top: "res4f_branch2b_div3"
}
layer {
  name: "res4f_branch2c_div3"
  type: "Convolution"
  bottom: "res4f_branch2b_div3"
  top: "res4f_branch2c_div3"
  param {
    name: "res4f_branch2c_weight"
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "bn4f_branch2c_div3"
  type: "BatchNorm"
  bottom: "res4f_branch2c_div3"
  top: "res4f_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale4f_branch2c_div3"
  type: "Scale"
  bottom: "res4f_branch2c_div3"
  top: "res4f_branch2c_div3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "res4f_div3"
  type: "Eltwise"
  bottom: "res4e_div3"
  bottom: "res4f_branch2c_div3"
  top: "res4f_div3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "res4f_relu_div3"
  type: "ReLU"
  bottom: "res4f_div3"
  top: "res4f_div3"
}



########## shared deep_mask feature on 16s ##########

layer { name: 'conv_feat_1_16s' type: 'Convolution' bottom: 'res4f' top: 'conv_feat_1_16s'
  param { name: "conv_feat_1" lr_mult: 1.0 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.00001 } bias_term: false } }

########## shared deep_mask feature on 24s ########

layer { name: 'conv_feat_1_24s' type: 'Convolution' bottom: 'res4f_div3' top: 'conv_feat_1_24s'
  param { name: "conv_feat_1" lr_mult: 1.0 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.00001 } bias_term: false } }

########## shared deep_mask feature on 32s ##########

layer { name: "conv5_4" type:"Convolution" bottom: "res4f" top: "conv5_4"
  param { name: 'rnn_conv_1_weight' } param { name: 'rnn_conv_1_bias' }
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv5_4_relu" type: "ReLU" bottom: "conv5_4" top: "conv5_4" }

layer { name: "conv5_5" type: "Convolution" bottom: "conv5_4" top: "conv5_5"
  param { name: 'rnn_conv_2_weight' } param { name: 'rnn_conv_2_bias' }
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 1024
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool5b" type: "Pooling" bottom: "conv5_5" top: "pool5b" 
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "pool5a" type: "Pooling" bottom: "res4f" top: "pool5a" 
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "sum_32s" type: "Eltwise" bottom: "pool5a" bottom: "pool5b" top: "sum_32s" }
layer { name: "sum_32s_relu" type: "ReLU" bottom: "sum_32s" top: "sum_32s" }

layer { name: "conv_feat_1_32s" type: "Convolution" bottom: "sum_32s" top: "conv_feat_1_32s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.00001 } bias_term: false } }

########## shared deep_mask feature on 48s ##########

layer { name: "conv5_4_div3" type:"Convolution" bottom: "res4f_div3" top: "conv5_4_div3"
  param { name: 'rnn_conv_1_weight' } param { name: 'rnn_conv_1_bias' }
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv5_4_div3_relu" type: "ReLU" bottom: "conv5_4_div3" top: "conv5_4_div3" }

layer { name: "conv5_5_div3" type: "Convolution" bottom: "conv5_4_div3" top: "conv5_5_div3"
  param { name: 'rnn_conv_2_weight' } param { name: 'rnn_conv_2_bias' }
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 1024
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool5b_div3" type: "Pooling" bottom: "conv5_5_div3" top: "pool5b_div3" 
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "pool5a_div3" type: "Pooling" bottom: "res4f_div3" top: "pool5a_div3" 
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "sum_48s" type: "Eltwise" bottom: "pool5a_div3" bottom: "pool5b_div3" top: "sum_48s" }
layer { name: "sum_48s_relu" type: "ReLU" bottom: "sum_48s" top: "sum_48s" }

layer { name: "conv_feat_1_48s" type: "Convolution" bottom: "sum_48s" top: "conv_feat_1_48s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.00001 } bias_term: false } }

########## shared deep_mask feature on 64s ##########

layer { name: "conv6_1" type:"Convolution" bottom: "sum_32s" top: "conv6_1"
  param { name: 'rnn_conv_1_weight' } param { name: 'rnn_conv_1_bias' }
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv6_1_relu" type: "ReLU" bottom: "conv6_1" top: "conv6_1" }

layer { name: "conv6_2" type:"Convolution" bottom: "conv6_1" top: "conv6_2"
  param { name: 'rnn_conv_2_weight' } param { name: 'rnn_conv_2_bias' }
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 1024
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool6b" type:"Pooling" bottom: "conv6_2" top: "pool6b"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "pool6a" type:"Pooling" bottom: "sum_32s" top: "pool6a"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "sum_64s" type: "Eltwise" bottom: "pool6a" bottom: "pool6b" top: "sum_64s" }
layer { name: "sum_64s_relu" type: "ReLU" bottom: "sum_64s" top: "sum_64s" }

layer { name: "conv_feat_1_64s" type: "Convolution" bottom: "sum_64s" top: "conv_feat_1_64s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

########## shared deep_mask feature on 96s ##########

layer { name: "conv6_1_div3" type:"Convolution" bottom: "sum_48s" top: "conv6_1_div3"
  param { name: 'rnn_conv_1_weight' } param { name: 'rnn_conv_1_bias' }
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv6_1_div3_relu" type: "ReLU" bottom: "conv6_1_div3" top: "conv6_1_div3" }

layer { name: "conv6_2_div3" type:"Convolution" bottom: "conv6_1_div3" top: "conv6_2_div3"
  param { name: 'rnn_conv_2_weight' } param { name: 'rnn_conv_2_bias' }
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 1024
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool6b_div3" type:"Pooling" bottom: "conv6_2_div3" top: "pool6b_div3"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "pool6a_div3" type:"Pooling" bottom: "sum_48s" top: "pool6a_div3"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "sum_96s" type: "Eltwise" bottom: "pool6a_div3" bottom: "pool6b_div3" top: "sum_96s" }
layer { name: "sum_96s_relu" type: "ReLU" bottom: "sum_96s" top: "sum_96s" }

layer { name: "conv_feat_1_96s" type: "Convolution" bottom: "sum_96s" top: "conv_feat_1_96s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }


########## shared deep_mask feature on 128s #########

layer { name: "conv7_1" type:"Convolution" bottom: "sum_64s" top: "conv7_1"
  param { name: 'rnn_conv_1_weight' } param { name: 'rnn_conv_1_bias' }
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv7_1_relu" type: "ReLU" bottom: "conv7_1" top: "conv7_1" }

layer { name: "conv7_2" type:"Convolution" bottom: "conv7_1" top: "conv7_2"
  param { name: 'rnn_conv_2_weight' } param { name: 'rnn_conv_2_bias' }
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 1024
    weight_filler { type: 'gaussian' std: 0.001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool7b" type:"Pooling" bottom: "conv7_2" top: "pool7b"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "pool7a" type:"Pooling" bottom: "sum_64s" top: "pool7a"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "sum_128s" type: "Eltwise" bottom: "pool7a" bottom: "pool7b" top: "sum_128s" }
layer { name: "sum_128s_relu" type: "ReLU" bottom: "sum_128s" top: "sum_128s" }

layer { name: "conv_feat_1_128s" type: "Convolution" bottom: "sum_128s" top: "conv_feat_1_128s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }


########## shared deep_mask feature on 192s #########

layer { name: "conv8_1_div3" type:"Convolution" bottom: "sum_96s" top: "conv8_1_div3"
  param { name: 'rnn_conv_1_weight' } param { name: 'rnn_conv_1_bias' }
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv8_1_div3_relu" type: "ReLU" bottom: "conv8_1_div3" top: "conv8_1_div3" }

layer { name: "conv8_2_div3" type:"Convolution" bottom: "conv8_1_div3" top: "conv8_2_div3"
  param { name: 'rnn_conv_2_weight' } param { name: 'rnn_conv_2_bias' }
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 1024
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool8b_div3" type:"Pooling" bottom: "conv8_2_div3" top: "pool8b_div3"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "pool8a_div3" type:"Pooling" bottom: "sum_96s" top: "pool8a_div3"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "sum_192s" type: "Eltwise" bottom: "pool8a_div3" bottom: "pool8b_div3" top: "sum_192s" }
layer { name: "sum_192s_relu" type: "ReLU" bottom: "sum_192s" top: "sum_192s" }

layer { name: "conv_feat_1_192s" type: "Convolution" bottom: "sum_192s" top: "conv_feat_1_192s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }





######### attSize 16 ##########

layer {  name: "res_buAttSize1skip_16"  type: "Convolution"  bottom: "conv_feat_1_16s"  top: "res_attSize1skip_16"
  param {    name: "res_attSize1skip_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1skip_16"  type: "Scale"  bottom: "res_attSize1skip_16"  top: "res_attSize1skip_16"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1a_16"  type: "Convolution"  bottom: "conv_feat_1_16s"  top: "res_attSize1a_16"
  param {    name: "res_attSize1a_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1a_16"  type: "Scale"  bottom: "res_attSize1a_16"  top: "res_attSize1a_16"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1a_16"  type: "ReLU"  bottom: "res_attSize1a_16"  top: "res_attSize1a_16"}
layer {  name: "res_buAttSize1b_16"  type: "Convolution"  bottom: "res_attSize1a_16"  top: "res_attSize1b_16"
  param {    name: "res_attSize1b_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1b_16"  type: "Scale"  bottom: "res_attSize1b_16"  top: "res_attSize1b_16"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1b_16"  type: "ReLU"  bottom: "res_attSize1b_16"  top: "res_attSize1b_16"}
layer {  name: "res_buAttSize1c_16"  type: "Convolution"    bottom: "res_attSize1b_16"  top: "res_attSize1c_16"
  param {    name: "res_attSize1c_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1c_16"  type: "Scale"  bottom: "res_attSize1c_16"  top: "res_attSize1c_16"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1_16"  type: "Eltwise"  bottom: "res_attSize1skip_16"  bottom: "res_attSize1c_16"  top: "res_attSize1_16"  eltwise_param {    operation: SUM  }}
layer {  name: "relu_buAttSize1_16"  type: "ReLU"  bottom: "res_attSize1_16" top: "res_attSize1_16"}

layer {  name: "res_buAttSize2_16"  type: "Convolution"    bottom: "res_attSize1_16"  top: "res_attSize2_16"
  param {    name: "res_attSize2_weight"  }
  convolution_param {    num_output: 1    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize2_16"  type: "Scale"  bottom: "res_attSize2_16"  top: "res_attSize2_16"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}

#layer { name: "loss_buAttSize_16" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "res_attSize2_16" bottom: "scaleMask_16" propagate_down: true propagate_down: false top: "loss_attSize_16" loss_weight: 1}
layer {  name: "sigmoid_buAttSize_16"  bottom: "res_attSize2_16"  top: "loss_attSize_16"  type: "Sigmoid"}


######### attSize 24 ##########

layer {  name: "res_buAttSize1skip_24"  type: "Convolution"  bottom: "conv_feat_1_24s"  top: "res_attSize1skip_24"
  param {    name: "res_attSize1skip_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1skip_24"  type: "Scale"  bottom: "res_attSize1skip_24"  top: "res_attSize1skip_24"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1a_24"  type: "Convolution"  bottom: "conv_feat_1_24s"  top: "res_attSize1a_24"
  param {    name: "res_attSize1a_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1a_24"  type: "Scale"  bottom: "res_attSize1a_24"  top: "res_attSize1a_24"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1a_24"  type: "ReLU"  bottom: "res_attSize1a_24"  top: "res_attSize1a_24"}
layer {  name: "res_buAttSize1b_24"  type: "Convolution"  bottom: "res_attSize1a_24"  top: "res_attSize1b_24"
  param {    name: "res_attSize1b_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1b_24"  type: "Scale"  bottom: "res_attSize1b_24"  top: "res_attSize1b_24"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1b_24"  type: "ReLU"  bottom: "res_attSize1b_24"  top: "res_attSize1b_24"}
layer {  name: "res_buAttSize1c_24"  type: "Convolution"    bottom: "res_attSize1b_24"  top: "res_attSize1c_24"
  param {    name: "res_attSize1c_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1c_24"  type: "Scale"  bottom: "res_attSize1c_24"  top: "res_attSize1c_24"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1_24"  type: "Eltwise"  bottom: "res_attSize1skip_24"  bottom: "res_attSize1c_24"  top: "res_attSize1_24"  eltwise_param {    operation: SUM  }}
layer {  name: "relu_buAttSize1_24"  type: "ReLU"  bottom: "res_attSize1_24" top: "res_attSize1_24"}

layer {  name: "res_buAttSize2_24"  type: "Convolution"    bottom: "res_attSize1_24"  top: "res_attSize2_24"
  param {    name: "res_attSize2_weight"  }
  convolution_param {    num_output: 1    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize2_24"  type: "Scale"  bottom: "res_attSize2_24"  top: "res_attSize2_24"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}

#layer { name: "loss_buAttSize_24" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "res_attSize2_24" bottom: "scaleMask_24" propagate_down: true propagate_down: false top: "loss_attSize_24" loss_weight: 1}
layer {  name: "sigmoid_buAttSize_24"  bottom: "res_attSize2_24"  top: "loss_attSize_24"  type: "Sigmoid"}

######### attSize 32 ##########

layer {  name: "res_buAttSize1skip_32"  type: "Convolution"  bottom: "conv_feat_1_32s"  top: "res_attSize1skip_32"
  param {    name: "res_attSize1skip_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1skip_32"  type: "Scale"  bottom: "res_attSize1skip_32"  top: "res_attSize1skip_32"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1a_32"  type: "Convolution"  bottom: "conv_feat_1_32s"  top: "res_attSize1a_32"
  param {    name: "res_attSize1a_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1a_32"  type: "Scale"  bottom: "res_attSize1a_32"  top: "res_attSize1a_32"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1a_32"  type: "ReLU"  bottom: "res_attSize1a_32"  top: "res_attSize1a_32"}
layer {  name: "res_buAttSize1b_32"  type: "Convolution"  bottom: "res_attSize1a_32"  top: "res_attSize1b_32"
  param {    name: "res_attSize1b_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1b_32"  type: "Scale"  bottom: "res_attSize1b_32"  top: "res_attSize1b_32"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1b_32"  type: "ReLU"  bottom: "res_attSize1b_32"  top: "res_attSize1b_32"}
layer {  name: "res_buAttSize1c_32"  type: "Convolution"    bottom: "res_attSize1b_32"  top: "res_attSize1c_32"
  param {    name: "res_attSize1c_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1c_32"  type: "Scale"  bottom: "res_attSize1c_32"  top: "res_attSize1c_32"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1_32"  type: "Eltwise"  bottom: "res_attSize1skip_32"  bottom: "res_attSize1c_32"  top: "res_attSize1_32"  eltwise_param {    operation: SUM  }}
layer {  name: "relu_buAttSize1_32"  type: "ReLU"  bottom: "res_attSize1_32" top: "res_attSize1_32"}

layer {  name: "res_buAttSize2_32"  type: "Convolution"    bottom: "res_attSize1_32"  top: "res_attSize2_32"
  param {    name: "res_attSize2_weight"  }
  convolution_param {    num_output: 1    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize2_32"  type: "Scale"  bottom: "res_attSize2_32"  top: "res_attSize2_32"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}

#layer { name: "loss_buAttSize_32" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "res_attSize2_32" bottom: "scaleMask_32" propagate_down: true propagate_down: false top: "loss_attSize_32" loss_weight: 1}
layer {  name: "sigmoid_buAttSize_32"  bottom: "res_attSize2_32"  top: "loss_attSize_32"  type: "Sigmoid"}


######### attSize 48 ##########

layer {  name: "res_buAttSize1skip_48"  type: "Convolution"  bottom: "conv_feat_1_48s"  top: "res_attSize1skip_48"
  param {    name: "res_attSize1skip_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1skip_48"  type: "Scale"  bottom: "res_attSize1skip_48"  top: "res_attSize1skip_48"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1a_48"  type: "Convolution"  bottom: "conv_feat_1_48s"  top: "res_attSize1a_48"
  param {    name: "res_attSize1a_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1a_48"  type: "Scale"  bottom: "res_attSize1a_48"  top: "res_attSize1a_48"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1a_48"  type: "ReLU"  bottom: "res_attSize1a_48"  top: "res_attSize1a_48"}
layer {  name: "res_buAttSize1b_48"  type: "Convolution"  bottom: "res_attSize1a_48"  top: "res_attSize1b_48"
  param {    name: "res_attSize1b_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1b_48"  type: "Scale"  bottom: "res_attSize1b_48"  top: "res_attSize1b_48"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1b_48"  type: "ReLU"  bottom: "res_attSize1b_48"  top: "res_attSize1b_48"}
layer {  name: "res_buAttSize1c_48"  type: "Convolution"    bottom: "res_attSize1b_48"  top: "res_attSize1c_48"
  param {    name: "res_attSize1c_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1c_48"  type: "Scale"  bottom: "res_attSize1c_48"  top: "res_attSize1c_48"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1_48"  type: "Eltwise"  bottom: "res_attSize1skip_48"  bottom: "res_attSize1c_48"  top: "res_attSize1_48"  eltwise_param {    operation: SUM  }}
layer {  name: "relu_buAttSize1_48"  type: "ReLU"  bottom: "res_attSize1_48" top: "res_attSize1_48"}

layer {  name: "res_buAttSize2_48"  type: "Convolution"    bottom: "res_attSize1_48"  top: "res_attSize2_48"
  param {    name: "res_attSize2_weight"  }
  convolution_param {    num_output: 1    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize2_48"  type: "Scale"  bottom: "res_attSize2_48"  top: "res_attSize2_48"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}

#layer { name: "loss_buAttSize_48" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "res_attSize2_48" bottom: "scaleMask_48" propagate_down: true propagate_down: false top: "loss_attSize_48" loss_weight: 1}
layer {  name: "sigmoid_buAttSize_48"  bottom: "res_attSize2_48"  top: "loss_attSize_48"  type: "Sigmoid"}


######### attSize 64 ##########

layer {  name: "res_buAttSize1skip_64"  type: "Convolution"  bottom: "conv_feat_1_64s"  top: "res_attSize1skip_64"
  param {    name: "res_attSize1skip_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1skip_64"  type: "Scale"  bottom: "res_attSize1skip_64"  top: "res_attSize1skip_64"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1a_64"  type: "Convolution"  bottom: "conv_feat_1_64s"  top: "res_attSize1a_64"
  param {    name: "res_attSize1a_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1a_64"  type: "Scale"  bottom: "res_attSize1a_64"  top: "res_attSize1a_64"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1a_64"  type: "ReLU"  bottom: "res_attSize1a_64"  top: "res_attSize1a_64"}
layer {  name: "res_buAttSize1b_64"  type: "Convolution"  bottom: "res_attSize1a_64"  top: "res_attSize1b_64"
  param {    name: "res_attSize1b_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1b_64"  type: "Scale"  bottom: "res_attSize1b_64"  top: "res_attSize1b_64"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1b_64"  type: "ReLU"  bottom: "res_attSize1b_64"  top: "res_attSize1b_64"}
layer {  name: "res_buAttSize1c_64"  type: "Convolution"    bottom: "res_attSize1b_64"  top: "res_attSize1c_64"
  param {    name: "res_attSize1c_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1c_64"  type: "Scale"  bottom: "res_attSize1c_64"  top: "res_attSize1c_64"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1_64"  type: "Eltwise"  bottom: "res_attSize1skip_64"  bottom: "res_attSize1c_64"  top: "res_attSize1_64"  eltwise_param {    operation: SUM  }}
layer {  name: "relu_buAttSize1_64"  type: "ReLU"  bottom: "res_attSize1_64" top: "res_attSize1_64"}

layer {  name: "res_buAttSize2_64"  type: "Convolution"    bottom: "res_attSize1_64"  top: "res_attSize2_64"
  param {    name: "res_attSize2_weight"  }
  convolution_param {    num_output: 1    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize2_64"  type: "Scale"  bottom: "res_attSize2_64"  top: "res_attSize2_64"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}

#layer { name: "loss_buAttSize_64" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "res_attSize2_64" bottom: "scaleMask_64" propagate_down: true propagate_down: false top: "loss_attSize_64" loss_weight: 1}
layer {  name: "sigmoid_buAttSize_64"  bottom: "res_attSize2_64"  top: "loss_attSize_64"  type: "Sigmoid"}

######### attSize 96 ##########

layer {  name: "res_buAttSize1skip_96"  type: "Convolution"  bottom: "conv_feat_1_96s"  top: "res_attSize1skip_96"
  param {    name: "res_attSize1skip_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1skip_96"  type: "Scale"  bottom: "res_attSize1skip_96"  top: "res_attSize1skip_96"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1a_96"  type: "Convolution"  bottom: "conv_feat_1_96s"  top: "res_attSize1a_96"
  param {    name: "res_attSize1a_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1a_96"  type: "Scale"  bottom: "res_attSize1a_96"  top: "res_attSize1a_96"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1a_96"  type: "ReLU"  bottom: "res_attSize1a_96"  top: "res_attSize1a_96"}
layer {  name: "res_buAttSize1b_96"  type: "Convolution"  bottom: "res_attSize1a_96"  top: "res_attSize1b_96"
  param {    name: "res_attSize1b_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1b_96"  type: "Scale"  bottom: "res_attSize1b_96"  top: "res_attSize1b_96"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1b_96"  type: "ReLU"  bottom: "res_attSize1b_96"  top: "res_attSize1b_96"}
layer {  name: "res_buAttSize1c_96"  type: "Convolution"    bottom: "res_attSize1b_96"  top: "res_attSize1c_96"
  param {    name: "res_attSize1c_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1c_96"  type: "Scale"  bottom: "res_attSize1c_96"  top: "res_attSize1c_96"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1_96"  type: "Eltwise"  bottom: "res_attSize1skip_96"  bottom: "res_attSize1c_96"  top: "res_attSize1_96"  eltwise_param {    operation: SUM  }}
layer {  name: "relu_buAttSize1_96"  type: "ReLU"  bottom: "res_attSize1_96" top: "res_attSize1_96"}

layer {  name: "res_buAttSize2_96"  type: "Convolution"    bottom: "res_attSize1_96"  top: "res_attSize2_96"
  param {    name: "res_attSize2_weight"  }
  convolution_param {    num_output: 1    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize2_96"  type: "Scale"  bottom: "res_attSize2_96"  top: "res_attSize2_96"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}

#layer { name: "loss_buAttSize_96" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "res_attSize2_96" bottom: "scaleMask_96" propagate_down: true propagate_down: false top: "loss_attSize_96" loss_weight: 1}
layer {  name: "sigmoid_buAttSize_96"  bottom: "res_attSize2_96"  top: "loss_attSize_96"  type: "Sigmoid"}

######### attSize 128 ##########

layer {  name: "res_buAttSize1skip_128"  type: "Convolution"  bottom: "conv_feat_1_128s"  top: "res_attSize1skip_128"
  param {    name: "res_attSize1skip_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1skip_128"  type: "Scale"  bottom: "res_attSize1skip_128"  top: "res_attSize1skip_128"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1a_128"  type: "Convolution"  bottom: "conv_feat_1_128s"  top: "res_attSize1a_128"
  param {    name: "res_attSize1a_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1a_128"  type: "Scale"  bottom: "res_attSize1a_128"  top: "res_attSize1a_128"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1a_128"  type: "ReLU"  bottom: "res_attSize1a_128"  top: "res_attSize1a_128"}
layer {  name: "res_buAttSize1b_128"  type: "Convolution"  bottom: "res_attSize1a_128"  top: "res_attSize1b_128"
  param {    name: "res_attSize1b_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1b_128"  type: "Scale"  bottom: "res_attSize1b_128"  top: "res_attSize1b_128"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1b_128"  type: "ReLU"  bottom: "res_attSize1b_128"  top: "res_attSize1b_128"}
layer {  name: "res_buAttSize1c_128"  type: "Convolution"    bottom: "res_attSize1b_128"  top: "res_attSize1c_128"
  param {    name: "res_attSize1c_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1c_128"  type: "Scale"  bottom: "res_attSize1c_128"  top: "res_attSize1c_128"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1_128"  type: "Eltwise"  bottom: "res_attSize1skip_128"  bottom: "res_attSize1c_128"  top: "res_attSize1_128"  eltwise_param {    operation: SUM  }}
layer {  name: "relu_buAttSize1_128"  type: "ReLU"  bottom: "res_attSize1_128" top: "res_attSize1_128"}

layer {  name: "res_buAttSize2_128"  type: "Convolution"    bottom: "res_attSize1_128"  top: "res_attSize2_128"
  param {    name: "res_attSize2_weight"  }
  convolution_param {    num_output: 1    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize2_128"  type: "Scale"  bottom: "res_attSize2_128"  top: "res_attSize2_128"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}

#layer { name: "loss_buAttSize_128" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "res_attSize2_128" bottom: "scaleMask_128" propagate_down: true propagate_down: false top: "loss_attSize_128" loss_weight: 1}
layer {  name: "sigmoid_buAttSize_128"  bottom: "res_attSize2_128"  top: "loss_attSize_128"  type: "Sigmoid"}

######### attSize 192 ##########

layer {  name: "res_buAttSize1skip_192"  type: "Convolution"  bottom: "conv_feat_1_192s"  top: "res_attSize1skip_192"
  param {    name: "res_attSize1skip_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1skip_192"  type: "Scale"  bottom: "res_attSize1skip_192"  top: "res_attSize1skip_192"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1a_192"  type: "Convolution"  bottom: "conv_feat_1_192s"  top: "res_attSize1a_192"
  param {    name: "res_attSize1a_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }}
}
layer {  name: "scale_buAttSize1a_192"  type: "Scale"  bottom: "res_attSize1a_192"  top: "res_attSize1a_192"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1a_192"  type: "ReLU"  bottom: "res_attSize1a_192"  top: "res_attSize1a_192"}
layer {  name: "res_buAttSize1b_192"  type: "Convolution"  bottom: "res_attSize1a_192"  top: "res_attSize1b_192"
  param {    name: "res_attSize1b_weight"  }
  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1b_192"  type: "Scale"  bottom: "res_attSize1b_192"  top: "res_attSize1b_192"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "relu_buAttSize1b_192"  type: "ReLU"  bottom: "res_attSize1b_192"  top: "res_attSize1b_192"}
layer {  name: "res_buAttSize1c_192"  type: "Convolution"    bottom: "res_attSize1b_192"  top: "res_attSize1c_192"
  param {    name: "res_attSize1c_weight"  }
  convolution_param {    num_output: 1024    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize1c_192"  type: "Scale"  bottom: "res_attSize1c_192"  top: "res_attSize1c_192"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}
layer {  name: "res_buAttSize1_192"  type: "Eltwise"  bottom: "res_attSize1skip_192"  bottom: "res_attSize1c_192"  top: "res_attSize1_192"  eltwise_param {    operation: SUM  }}
layer {  name: "relu_buAttSize1_192"  type: "ReLU"  bottom: "res_attSize1_192" top: "res_attSize1_192"}

layer {  name: "res_buAttSize2_192"  type: "Convolution"    bottom: "res_attSize1_192"  top: "res_attSize2_192"
  param {    name: "res_attSize2_weight"  }
  convolution_param {    num_output: 1    bias_term: false    pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.001    }    bias_filler {      type: "constant"      value: 0.01    }  }
}
layer {  name: "scale_buAttSize2_192"  type: "Scale"  bottom: "res_attSize2_192"  top: "res_attSize2_192"
  param {    lr_mult: 0  }  param {    lr_mult: 0  }  scale_param {    bias_term: true  }
}

#layer { name: "loss_buAttSize_192" type: "NormalizedSigmoidCrossEntropyLoss" bottom: "res_attSize2_192" bottom: "scaleMask_192" propagate_down: true propagate_down: false top: "loss_attSize_192" loss_weight: 1}
layer {  name: "sigmoid_buAttSize_192"  bottom: "res_attSize2_192"  top: "loss_attSize_192"  type: "Sigmoid"}



########## sample #########
layer { name: "extractor" type: "SlidingWindow" bottom: "conv_feat_1_16s", top: "sample_16s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "extractor" type: "SlidingWindow" bottom: "conv_feat_1_24s" top: "sample_24s" 
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "extractor" type: "SlidingWindow" bottom: "conv_feat_1_32s", top: "sample_32s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "extractor" type: "SlidingWindow" bottom: "conv_feat_1_48s", top: "sample_48s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "extractor" type: "SlidingWindow" bottom: "conv_feat_1_64s", top: "sample_64s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "extractor" type: "SlidingWindow" bottom: "conv_feat_1_96s", top: "sample_96s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "extractor" type: "SlidingWindow" bottom: "conv_feat_1_128s", top: "sample_128s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "extractor" type: "SlidingWindow" bottom: "conv_feat_1_192s", top: "sample_192s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "sample_concat" type: "Concat" 
  bottom: "sample_16s" bottom: "sample_24s" bottom: "sample_32s" bottom: "sample_48s"
  bottom: "sample_64s" bottom: "sample_96s" bottom: "sample_128s" bottom: "sample_192s" top: "sample"
  concat_param { concat_dim: 0 } }

layer { bottom: "sample" top: "sample" type: "BatchNorm" name:"sample_bn" batch_norm_param { use_global_stats: true } 
  param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } }
layer { bottom: "sample" top: "sample" type: "Scale" name: "sample_scale" scale_param { bias_term: true } 
  param { lr_mult: 1 decay_mult: 0 } param { lr_mult: 1 decay_mult: 0} }

########## cls branch ##########

layer { name: 'cls_1' type: 'InnerProduct' bottom: 'sample' top: 'cls_1'
    param { lr_mult: 1.0 } param { lr_mult: 1.0 }
    inner_product_param { num_output: 512 weight_filler: { type: 'gaussian' std: 0.01 } } }
layer { name: 'relu_cls_1' type: 'ReLU' bottom: 'cls_1' top: 'cls_1' }
layer { name: 'dropout_cls_1' type: 'Dropout' bottom: 'cls_1' top: 'cls_1' dropout_param { dropout_ratio: 0.5 } }


layer { name: 'cls_2' type: 'InnerProduct' bottom: 'cls_1' top: 'cls_2'
  param { lr_mult: 1.0 } param { lr_mult: 1.0 }
  inner_product_param { num_output: 1024 weight_filler: { type: 'gaussian' std: 0.01 } } }
layer { name: 'relu_cls_2' type: 'ReLU' bottom: 'cls_2' top: 'cls_2' }


########## cls score ##########

layer { name: 'obj_score' type: 'InnerProduct' bottom: 'cls_2' top: 'obj_score' 
  param { lr_mult: 1.0 } param { lr_mult: 2.0 }
  inner_product_param { num_output: 1 weight_filler { type: "gaussian" std: 0.0001 }  bias_filler { type: "constant" std: 0 } } }
layer { name: 'sig_score' type: 'Sigmoid' bottom: 'obj_score' top: 'objn' }

layer { name: "top_k" type: "DummyData" top: "k" dummy_data_param { shape { dim: 100 dim: 1 dim: 1 dim: 1} } }

layer { name: "batch_filter" type: "TopK" bottom: "sample" bottom: "objn" bottom: "k" top: "filted_sample" top: "top_k" }

########## att branch ##########
layer { name: 'att' type: 'InnerProduct' bottom: 'filted_sample' top: 'flatten_att'
  param { lr_mult: 1.0 } param { lr_mult: 2.0 }
  inner_product_param { num_output: 100 weight_filler: { type: 'gaussian' std: 0.0001 } bias_filler { type: 'constant' value: 0 } } }

layer {
  name: "atts_reshape" type: "Reshape" bottom: "flatten_att" top: "atts"
  reshape_param { shape { dim: -1 dim: 1 dim: 10, dim: 10 } }
}


######### att filter ###########
layer { name: 'att_sigmoid' type: 'Sigmoid' bottom: 'atts' top: 'sig_atts' }
layer { name: 'att_filt' type: 'TileProduct' bottom: 'filted_sample' bottom: 'sig_atts' top: 'atts_filt_feat' }


layer {
  name: "silence_layer"
  type: "Silence"
  bottom: "loss_attSize_16"
  bottom: "loss_attSize_24"
  bottom: "loss_attSize_32"
  bottom: "loss_attSize_48"
  bottom: "loss_attSize_64"
  bottom: "loss_attSize_96"
  bottom: "loss_attSize_128"
  bottom: "loss_attSize_192"
  bottom: "top_k"
}
